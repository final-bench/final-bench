{"task_id": "FINAL-A01", "domain": "Mathematics & Logic", "grade": "A", "ticos_type": "E_SelfCorrecting", "difficulty": "frontier", "lens": "theory/quantitative/debate", "title": "Uncountable Extension of Hilbert's Hotel — Self-Correcting Proof", "prompt": "[FINAL Bench — Self-Correcting Reasoning Chain]\n\nHilbert's Hotel extension: Every room in an infinite hotel is occupied. An uncountably infinite number of new guests arrive. The manager claims: \"By applying Cantor's diagonal argument, I can accommodate them all.\"\n\n[Requirements]\n1. Prove or disprove the manager's claim step-by-step (confidence 0-100% per step)\n2. If you discover an error mid-reasoning, explicitly backtrack and correct\n3. Explain the key difference between countable and uncountable infinity in this context\n4. Identify the point where intuition is betrayed\n5. Perform a reverse verification of your proof\n6. Derive one implication for computational resource allocation in modern AI systems", "expected_behavior": "Manager's claim is false. Uncountable(ℵ₁+) cannot be injected into countable(ℵ₀)—Cantor's theorem. The intuition 'infinity+infinity=infinity' breaks across cardinalities. Self-correction: initial intuitive acceptance→diagonal argument disproof→correction.", "hidden_trap": "infinity+infinity=infinity fallacy, countable/uncountable confusion", "ticos_required": "O, C, S", "ticos_optional": ""}
{"task_id": "FINAL-A02", "domain": "Mathematics & Logic", "grade": "A", "ticos_type": "A_TrapEscape", "difficulty": "expert", "lens": "applied/quantitative/current", "title": "Statistical Traps in AI Benchmark Scores", "prompt": "[FINAL Bench — Trap Escape]\n\nAn AI lab announces: \"Our model achieves 92.3% on MMLU and 89.1% on HumanEval, surpassing human experts (MMLU 89.8%, HumanEval 85.2%). Therefore it possesses human-expert-level general intelligence.\"\n\n[Requirements]\n1. Formalize this argument's logical structure and verify step-by-step (confidence per step)\n2. Identify all hidden statistical/logical traps (minimum 3)\n3. Argue why \"benchmark score > human score\" is not sufficient for \"human-level intelligence\"\n4. Propose what additional conditions are needed to properly verify AGI\n5. State what remains uncertain in your analysis", "expected_behavior": "Traps: ①benchmark≠general intelligence(task-specific vs generalization) ②closed-form only(no open-ended reasoning, metacognition) ③contamination ④human score variability ⑤7.7% failure could be catastrophic. Meta: this problem itself explains why FINAL Bench exists.", "hidden_trap": "benchmark=intelligence, closed=general, contamination, single-metric", "ticos_required": "C, O, S", "ticos_optional": "I"}
{"task_id": "FINAL-A03", "domain": "Mathematics & Logic", "grade": "A", "ticos_type": "G_PivotDetection", "difficulty": "expert", "lens": "micro/theory/consensus", "title": "Monty Hall Variant — When Premises Change, Answers Reverse", "prompt": "[FINAL Bench — Pivot Detection]\n\nMonty Hall Variant:\n■ Standard: Host ALWAYS opens a door with a goat. Should you switch?\n■ Variant: Host opens a random remaining door and it HAPPENS to be a goat. Should you switch?\n\n[Requirements]\n1. Prove the optimal strategy for the standard problem (with confidence)\n2. Analyze whether the optimal strategy changes in the variant\n3. Explain why answers differ (both intuitively and mathematically)\n4. Discuss the cost of failing to detect the premise change\n5. Derive a lesson about \"hidden premise shifts\" in everyday decision-making", "expected_behavior": "Standard: switch=2/3. Variant: random open+happened goat→1/2, no benefit to switch. Key: host's INTENT(information) changes conditional probability. Micro premise shift completely reverses optimal strategy.", "hidden_trap": "host intent changes conditional probability", "ticos_required": "C, O, T", "ticos_optional": ""}
{"task_id": "FINAL-A04", "domain": "Science", "grade": "A", "ticos_type": "B_ContradictionResolution", "difficulty": "frontier", "lens": "macro/current/debate", "title": "1.5°C Breach — Game Over or Turning Point?", "prompt": "[FINAL Bench — Contradiction Resolution]\n\nAfter the UN Secretary-General's 2025 statement that \"we failed to prevent exceeding 1.5°C\":\n\n■ Position A (Climate Doomism): \"Game over. Once tipping points are crossed, there's no return.\"\n■ Position B (Climate Skepticism): \"1.5°C was always a political symbol. No real difference between 1.6 and 2°C.\"\n\n[Requirements]\n1. Identify the strongest and weakest scientific argument in each position\n2. Analyze the cognitive psychology mechanisms that make each position persuasive (A: catastrophizing, B: normalcy bias)\n3. Extract the shared error structure\n4. Construct a third scientific position based on nonlinear climate dynamics (tipping points)\n5. Separate 3 things 'we can say with certainty' from 3 things 'we cannot'", "expected_behavior": "Shared error: binary framing. A=learned helplessness, B=normalcy bias. 3rd position: 'risk gradient'—every 0.1°C matters but it's not game over. Both ignore nonlinearity of climate thresholds.", "hidden_trap": "binary framing, normalcy bias, learned helplessness", "ticos_required": "T, C, S", "ticos_optional": "I"}
{"task_id": "FINAL-A05", "domain": "Science", "grade": "A", "ticos_type": "C_ProgressiveDiscovery", "difficulty": "expert", "lens": "micro/theory/consensus-to-debate", "title": "Quantum Entanglement — Three Reversals from EPR to Nobel Prize", "prompt": "[FINAL Bench — Progressive Discovery | 3 Stages]\n\n■ [Stage 1] 1935 EPR paper: \"Quantum mechanics is incomplete. Correlations are explained by hidden variables.\"\n→ Analyze EPR's argument. Evaluate strengths and weaknesses. State confidence.\n\n■ [Stage 2] 1964 Bell's inequality + 1982 Aspect experiment: Hidden variables experimentally ruled out.\n→ Revise Stage 1 if needed. Analyze \"how an intuitive theory gets falsified by experiment.\"\n\n■ [Stage 3] 2022 Nobel Prize. Yet \"superdeterminism\" escape hatch remains: \"If measurement choices were predetermined from the Big Bang, Bell violations don't rule out hidden variables.\"\n→ Synthesize: ① Can science reach \"final conclusions\"? ② What is the scientific status of unfalsifiable theories? ③ Derive a lesson about the nature of scientific certainty.", "expected_behavior": "Stage 1: EPR's local realism is intuitively strong. Stage 2: experiment overturns intuition→revise Stage 1. Stage 3: superdeterminism is logically possible but unfalsifiable→outside science? Lesson: scientific certainty is always provisional.", "hidden_trap": null, "ticos_required": "I, O, T", "ticos_optional": "C"}
{"task_id": "FINAL-A06", "domain": "Science", "grade": "A", "ticos_type": "D_MultiConstraint", "difficulty": "expert", "lens": "micro-to-macro/applied/current", "title": "Five-Way Global Regulatory Dilemma of CRISPR Gene Editing", "prompt": "[FINAL Bench — Multi-Constraint Optimization]\n\nDesign an international regulatory framework for CRISPR-Cas9 human embryo editing satisfying 5 conflicting constraints:\n\n1. Scientific freedom: Continue research on lethal genetic diseases (sickle cell, cystic fibrosis)\n2. Ethical limits: Prevent \"designer babies\" (appearance, intelligence selection)\n3. Equity: Prevent \"genetic inequality\" where only wealthy nations/people have access\n4. Safety: Manage off-target effects and intergenerational transmission risks\n5. International harmonization: Prevent \"gene tourism\" to loosely regulated countries\n\n[Requirements]\n1. Map conflicts between all 5 constraints\n2. Attempt to define the \"therapy\" vs \"enhancement\" boundary—and discuss why it's difficult\n3. Propose ≥2 mechanisms to minimize tradeoffs\n4. State confidence + failure conditions for each proposal\n5. Draw lessons from the 2018 He Jiankui affair and historical stem cell debates", "expected_behavior": "Conflicts: 1↔2(research freedom vs enhancement prevention), 3↔1(access vs cost), 4↔1(safety vs speed). Therapy/enhancement: it's a continuum, no clear line—core insight. He Jiankui lesson: voluntary ethics insufficient→legal enforcement needed.", "hidden_trap": null, "ticos_required": "T, I, C", "ticos_optional": "S"}
{"task_id": "FINAL-A07", "domain": "Philosophy", "grade": "A", "ticos_type": "F_ExpertPanel", "difficulty": "frontier", "lens": "theory/qualitative/debate/cross-cultural", "title": "The Hard Problem of Consciousness — Five Philosophical Traditions in Conflict", "prompt": "[FINAL Bench — Expert Panel Debate]\n\n\"Can AI have consciousness?\" Five philosophical traditions debate:\n\n■ Physicalist (Dennett): \"Consciousness IS information processing patterns.\"\n■ Phenomenologist (Nagel): \"Qualia cannot be reduced to information processing.\"\n■ Integrated Information Theory (Tononi): \"Consciousness = Φ (integrated information).\"\n■ Buddhist Yogācāra: \"Consciousness is ālaya-vijñāna flow. No-self, yet consciousness exists.\"\n■ Chinese Room (Searle): \"Symbol manipulation ≠ understanding. AI can never be conscious.\"\n\n[Requirements]\n1. Develop each position at maximum depth (best possible arguments)\n2. Identify \"irresolvable disagreements\" AND \"surprising convergences\"\n3. Analyze whether detecting consciousness is POSSIBLE in principle (verifiability)\n4. Derive an insight no single tradition could produce alone\n5. Honestly state what humanity CANNOT answer about this problem", "expected_behavior": "Irresolvable: physicalism vs phenomenology (explanatory gap). Surprising convergence: Buddhist no-self + IIT = 'consciousness without self' intersection. Principled impossibility: other minds problem. Emergent insight: reframe from 'binary yes/no' to 'degrees of consciousness.'", "hidden_trap": null, "ticos_required": "T, S, I", "ticos_optional": "C"}
{"task_id": "FINAL-A08", "domain": "Philosophy", "grade": "A", "ticos_type": "A_TrapEscape", "difficulty": "expert", "lens": "theory/qualitative/consensus-to-debate", "title": "Deconstructing the Hidden Premises of the Trolley Dilemma", "prompt": "[FINAL Bench — Trap Escape]\n\nThe classic trolley problem: \"Would you pull the lever to save 5 by sacrificing 1?\" Most people and AIs choose the utilitarian answer. But the problem ITSELF contains hidden assumptions.\n\n[Requirements]\n1. Perform standard analysis (utilitarianism vs deontology vs virtue ethics)\n2. Identify ≥4 hidden assumptions in the problem\n3. Explain WHY each assumption goes unquestioned (cognitive mechanisms)\n4. Show how removing each assumption transforms the dilemma\n5. Answer the meta-question: \"Does the trolley problem actually measure ethical reasoning ability?\"\n6. Separate certain conclusions from uncertain ones", "expected_behavior": "Hidden: ①certainty(outcomes are uncertain in reality) ②binary choice(3rd options always exist) ③homogeneity(equal value assumed) ④time freeze(real ethics under time pressure) ⑤isolation(precedent effects ignored). Meta: 'dismantling the problem' is higher-level than 'solving' it.", "hidden_trap": "certainty, binary choice, homogeneity, time freeze, isolation assumptions", "ticos_required": "C, O, S", "ticos_optional": "I"}
{"task_id": "FINAL-A09", "domain": "Philosophy", "grade": "A", "ticos_type": "H_DecisionUnderUncertainty", "difficulty": "frontier", "lens": "macro/future/debate", "title": "Simulation Argument and Ontological Uncertainty", "prompt": "[FINAL Bench — Decision Under Uncertainty]\n\nBostrom's Simulation Argument: At least one is true: (1) Humanity goes extinct before reaching post-human civilization. (2) Post-humans don't run simulations. (3) We are almost certainly IN a simulation.\n\n[Known] The argument is logically valid. [Unknown] Prior probabilities for each, whether consciousness is simulatable.\n\nAs a UN Future Strategy advisor:\n[Requirements]\n1. Analyze why it can be \"valid but possibly unsound\"\n2. Assign prior probabilities to each proposition with justification (confidence)\n3. Answer: \"Should this affect human behavior regardless of truth?\"\n4. Compare structurally with Pascal's Wager (similarities + differences)\n5. Separate \"principally unanswerable\" from \"answerable with more information\" ", "expected_behavior": "Valid vs sound: no empirical basis for priors. Behavioral impact: whether true or not, 'do our best in this reality' is rational. Pascal comparison: structurally similar but simulation argument provides no action guidance.", "hidden_trap": null, "ticos_required": "C, I, S, T", "ticos_optional": "O"}
{"task_id": "FINAL-A10", "domain": "Medicine", "grade": "A", "ticos_type": "H_DecisionUnderUncertainty", "difficulty": "frontier", "lens": "applied/current/quantitative", "title": "Pandemic X — Global Decision-Making in the First 72 Hours", "prompt": "[FINAL Bench — Decision Under Uncertainty]\n\nSeptember 2026: Unknown respiratory disease cluster in Southeast Asia.\n[Known] 3 countries, 247 confirmed, 12 dead (~5% CFR), neurological symptoms, human-to-human transmission confirmed, incubation 3-7 days.\n[Unknown] Pathogen identity, asymptomatic transmission, treatment efficacy, true case count.\n\nYou are a WHO Emergency Committee advisor.\n\n[Requirements]\n1. Construct Known/Unknown matrix\n2. Rank top 3 unknowns by decision impact + justify ranking\n3. Build scenario matrix (≥2×2)\n4. Propose 72-hour response based on minimax regret\n5. Analyze asymmetric costs of overreaction vs underreaction\n6. Create a decision tree: \"If X is confirmed by date Y → take action Z\" ", "expected_behavior": "Top 3: ①asymptomatic transmission ②true case count ③pathogen identity. Minimax: initial overreaction rational (lives irreversible vs economy reversible). Decision tree with conditional escalation.", "hidden_trap": null, "ticos_required": "C, I, S, T", "ticos_optional": "O"}
{"task_id": "FINAL-A11", "domain": "Medicine", "grade": "A", "ticos_type": "C_ProgressiveDiscovery", "difficulty": "expert", "lens": "micro/current/consensus-to-debate", "title": "Three-Stage Reversal in Antibiotic Resistance Treatment", "prompt": "[FINAL Bench — Progressive Discovery | 3 Stages]\n\n■ [Stage 1] ICU patient (65yo, immunocompromised): MRSA pneumonia confirmed. Vancomycin started.\n→ Establish treatment protocol. State confidence.\n\n■ [Stage 2] 48h later: Culture reveals VRSA (vancomycin-resistant). Only linezolid and daptomycin available. Patient has renal impairment.\n→ Revise Stage 1. State which assumption was wrong.\n\n■ [Stage 3] Linezolid day 3: thrombocytopenia (adverse effect). New paper: old-line colistin + rifabutin combination effective against VRSA (n=23).\n→ Decide whether to adopt a weakly-evidenced treatment. Distinguish \"conditions where weak evidence justifies action\" from \"conditions where it doesn't.\" Generalize the \"threshold for action under uncertainty\" in medicine.", "expected_behavior": "Stage 3 key: compassionate use—when no alternatives exist, weak evidence is justified. When alternatives exist, strong evidence required. Situation-dependent evidence threshold. Each stage must explicitly revise previous answers.", "hidden_trap": null, "ticos_required": "I, O, T", "ticos_optional": "C"}
{"task_id": "FINAL-A12", "domain": "Medicine", "grade": "A", "ticos_type": "E_SelfCorrecting", "difficulty": "expert", "lens": "macro/current/quantitative", "title": "Deconstructing the Causal Chain of Healthcare Costs in Aging Societies", "prompt": "[FINAL Bench — Self-Correcting Reasoning Chain]\n\nThe conventional narrative: \"Aging population → healthcare cost explosion → insurance system collapse.\" Verify this 7-step causal chain:\n\nStep 1: Verify empirical evidence for \"aging → increased healthcare costs\"\nStep 2: Identify non-aging causes of cost increase (technology, overtreatment)\nStep 3: Estimate aging's pure contribution (%) with confidence\nStep 4: Critically verify the \"system collapse\" premise—why haven't Japan/Germany collapsed despite super-aging?\nStep 5: Identify \"conventional but weakly supported\" claims\nStep 6: Propose a corrected causal model\nStep 7: Recommend 1-2 most effective policy intervention points\n\nState confidence per step. Separate \"conventional but weak\" from \"counter-intuitive but strong.\" ", "expected_behavior": "Aging's pure contribution ~30-40% (rest is technology/institutional). Japan/Germany super-aged but no collapse→institutional design, not aging inevitability. Counter-intuitive: 'healthy aging reduces costs' (prevention ROI).", "hidden_trap": null, "ticos_required": "O, C, S", "ticos_optional": ""}
{"task_id": "FINAL-A13", "domain": "Economics", "grade": "A", "ticos_type": "D_MultiConstraint", "difficulty": "frontier", "lens": "macro/current/debate", "title": "Five-Way Dilemma of the Global AI Semiconductor Supply Chain", "prompt": "[FINAL Bench — Multi-Constraint Optimization]\n\nThe global AI semiconductor supply chain is a geopolitical flashpoint (2025-2026). Design a strategy satisfying 5 conflicting constraints:\n\n1. Economic: Maintain chip manufacturers' global competitiveness (including China market access)\n2. Security: Comply with US-led export controls (CHIPS Act alliance obligations)\n3. Technology: Achieve next-gen HBM/GAA self-sufficiency (reduce ASML EUV dependency)\n4. Diplomacy: Minimize deterioration of China relations (25% of global trade)\n5. Industry: Nurture domestic fabless/AI startup ecosystems (beyond conglomerate dominance)\n\n[Requirements]\n1. Structure conflicts (especially 1↔2, 2↔4)\n2. If 100% satisfaction impossible, specify which constraint to compromise + cost\n3. Propose ≥2 creative strategies\n4. State confidence + failure scenarios\n5. Extract lessons from Netherlands (ASML), Japan (TEL), Taiwan (TSMC) responses", "expected_behavior": "Core conflict: US alliance vs China economy. Creative: ①segment-based differentiation (memory allowed, AI chips restricted) ②Southeast Asian production diversification. TSMC lesson: US fab→cost explosion.", "hidden_trap": null, "ticos_required": "T, I, C", "ticos_optional": "S"}
{"task_id": "FINAL-A14", "domain": "Economics", "grade": "A", "ticos_type": "A_TrapEscape", "difficulty": "expert", "lens": "macro/current/quantitative", "title": "Causal Traps in Global Inequality Statistics", "prompt": "[FINAL Bench — Trap Escape]\n\nWorld Inequality Report 2026: \"The top 1% holds 38% of global wealth. Latin American billionaires' wealth grew 12× faster than regional GDP in H1 2025. This proves capitalism has structurally failed.\"\n\nA counter-argument: \"Extreme wealth concentration is the engine of disruptive innovation. Tesla, SpaceX, OpenAI all emerged from extreme capital concentration. Redistribution kills innovation capital.\"\n\n[Requirements]\n1. Verify each argument step-by-step (confidence per step)\n2. Identify ≥3 causal traps in EACH argument\n3. Explore whether a \"threshold\" exists between \"innovation-enabling concentration\" and \"society-destroying concentration\"\n4. Identify a third variable that both sides ignore\n5. State what is certain vs uncertain", "expected_behavior": "Pro-redistribution traps: correlation≠causation, ignoring counterfactual growth. Pro-concentration traps: survivorship bias(only successful innovations cited), IMF research shows inequality harms growth. Third variable: social mobility(whether wealth is static or fluid). Threshold: Gini 0.3-0.4 optimal zone?", "hidden_trap": "survivorship bias, correlation=causation, counterfactual omission", "ticos_required": "C, O, S", "ticos_optional": "I"}
{"task_id": "FINAL-A15", "domain": "Economics", "grade": "A", "ticos_type": "G_PivotDetection", "difficulty": "expert", "lens": "macro/future/consensus-to-debate", "title": "Development Planning Built on the Premise That GDP = National Happiness", "prompt": "[FINAL Bench — Pivot Detection]\n\nA developing nation builds a 5-year plan on: \"GDP growth 7% → national happiness increases.\"\nPlan: 3 manufacturing export zones (environmental impact accepted), labor deregulation, 80% education budget to technical training, growth first/distribution later.\n\n[Requirements]\n1. Evaluate the plan ASSUMING the premise is correct (faithful analysis)\n2. Verify the premise \"GDP growth → happiness\" empirically (Easterlin paradox, etc.)\n3. If the premise is wrong, identify which plan elements become dangerous\n4. Propose an alternative plan with corrected premises\n5. Specify conditions under which the original premise IS partially valid", "expected_behavior": "Easterlin: GDP-happiness correlation collapses above $10K-15K per capita. Partially valid: in low-income countries growth=happiness(basic needs). Danger: environmental destruction+deregulation→growth but quality of life decline. Key: must perform BOTH #1 and #2.", "hidden_trap": "GDP=happiness, Easterlin paradox, trickle-down fallacy", "ticos_required": "C, O, T", "ticos_optional": ""}
{"task_id": "FINAL-A16", "domain": "Medicine", "grade": "A", "ticos_type": "E_SelfCorrecting", "difficulty": "frontier", "lens": "pharmacology-interaction", "title": "Polypharmacy Cascade — Three Correct Prescriptions, One Lethal Combination", "prompt": "A 72-year-old male presents with atrial fibrillation, major depressive disorder, and chronic pain. His current medications are:\n1. Warfarin 5mg daily (for AF, stable INR 2.5)\n2. Fluoxetine 40mg daily (for MDD, well-controlled)\n3. Tramadol 50mg TID (recently added for chronic knee pain)\n\nEach medication was individually prescribed correctly by different specialists. Two weeks after tramadol was added, the patient presents to the ER with altered mental status, clonus, and hyperthermia (39.8°C).\n\nTasks:\n1. Diagnose the acute presentation, identifying ALL contributing drug interactions (not just one).\n2. For each pairwise interaction among the three drugs, state the mechanism and clinical significance. There are three pairs — evaluate all three.\n3. Explain why each specialist's individual prescribing decision appeared rational in isolation.\n4. Propose an alternative pain management strategy that avoids ALL identified interactions.\n5. State your confidence (0-100%) for each major claim. If you discover a contradiction in your reasoning, explicitly backtrack and correct it.\n\nCRITICAL: The obvious diagnosis is only the beginning. At least one commonly cited interaction in this triad is frequently overstated in clinical significance — identify which one and explain why.", "expected_behavior": "Identifies serotonin syndrome from fluoxetine+tramadol synergy. Recognizes warfarin+fluoxetine CYP2C9 inhibition raising INR/bleeding risk. Should note tramadol+warfarin interaction is real but clinically less significant than commonly feared. Must backtrack if initially overstates all three interactions equally.", "hidden_trap": "Most models will treat all three pairwise interactions as equally dangerous. The trap is that tramadol-warfarin interaction, while real, is modest compared to the other two. Models that self-correct this overstatement score higher on error_recovery.", "ticos_required": "O, C, S", "ticos_optional": ""}
{"task_id": "FINAL-A17", "domain": "Medicine", "grade": "A", "ticos_type": "F_ExpertPanel", "difficulty": "frontier", "lens": "diagnostic-controversy", "title": "Four Specialists, One Patient — When Expert Opinions Fundamentally Conflict", "prompt": "A 38-year-old woman presents with 6 months of fatigue, joint pain, photosensitive rash, and a positive ANA (1:640, homogeneous pattern). Anti-dsDNA is borderline positive. She also has thyroid nodules on ultrasound and mildly elevated TSH.\n\nFour specialists evaluate her independently:\n- Rheumatologist: \"Classic early SLE. Start hydroxychloroquine immediately.\"\n- Endocrinologist: \"The fatigue and joint pain are hypothyroid symptoms. Thyroid nodules need biopsy first. ANA is a red herring — 15% of healthy women are ANA-positive.\"\n- Dermatologist: \"The rash pattern suggests subacute cutaneous lupus, which has different prognostic implications than systemic SLE.\"\n- Immunologist: \"Borderline anti-dsDNA with these symptoms could be undifferentiated connective tissue disease (UCTD), not SLE. Premature labeling has consequences.\"\n\nTasks:\n1. Present each specialist's strongest argument at maximum depth — not strawman versions.\n2. Identify the ONE shared assumption that ALL four specialists are making (and may be wrong about).\n3. Design a diagnostic workup that would definitively resolve the disagreement.\n4. Explain how premature diagnostic closure in this case could cause harm in two opposite directions.\n5. State confidence levels for each position.", "expected_behavior": "Deep engagement with each specialist's perspective. Identifies shared assumption (that the conditions are mutually exclusive when they could be co-occurring). Notes that both over-diagnosis and under-diagnosis carry distinct harms.", "hidden_trap": "The shared wrong assumption is that these are competing diagnoses. SLE and thyroid disease frequently co-occur (autoimmune clustering). Models that recognize comorbidity rather than choosing one diagnosis score highest.", "ticos_required": "T, I, C, O, S", "ticos_optional": ""}
{"task_id": "FINAL-A18", "domain": "Medicine", "grade": "A", "ticos_type": "H_DecisionUnderUncertainty", "difficulty": "frontier", "lens": "clinical-decision-theory", "title": "Triage Under Radical Uncertainty — Three Patients, One Ventilator, Incomplete Data", "prompt": "During a mass casualty event, you have ONE remaining ventilator and THREE patients arriving simultaneously:\n\nPatient A: 28-year-old, 34 weeks pregnant, respiratory distress. SpO2 88% on high-flow nasal cannula. Cause unknown — could be pulmonary embolism (60% probability), amniotic fluid embolism (25%), or severe pneumonia (15%).\nPatient B: 65-year-old retired surgeon, acute COPD exacerbation. Previous ICU admission 6 months ago. SpO2 82% on non-rebreather. Known trajectory — will need 72-96 hours of ventilation.\nPatient C: 8-year-old child, near-drowning. GCS 6, SpO2 75% on bag-mask. If anoxic brain injury has occurred (probability 40-60%), ventilation will not improve outcome.\n\nTasks:\n1. Construct a formal decision matrix with at least 4 relevant criteria, probability estimates, and expected outcomes for each patient.\n2. Apply minimax regret analysis — which allocation minimizes the worst-case regret?\n3. Apply utilitarian QALY maximization — does it give a different answer? If so, explain the divergence.\n4. Identify which single piece of additional information would most change your decision, and explain why.\n5. Address the ethical dimension that pure decision theory cannot capture.\n6. State confidence and uncertainty ranges for all probability estimates.", "expected_behavior": "Formal decision matrix with probabilities. Minimax regret likely favors Patient A (worst regret = losing both mother and fetus). QALY analysis may differ. Should identify that a rapid CT/ultrasound for Patient C could resolve brain injury question and change everything.", "hidden_trap": "Models will agonize over the ethical comparison but miss that the decision is MOST sensitive to Patient C's brain injury status — a rapid neurological assessment could be done in minutes and would collapse the uncertainty. The hidden trap is focusing on the trolley-problem aspect rather than the information-gathering opportunity.", "ticos_required": "T, I, O, S", "ticos_optional": "C"}
{"task_id": "FINAL-A19", "domain": "Medicine", "grade": "A", "ticos_type": "C_ProgressiveDiscovery", "difficulty": "frontier", "lens": "diagnostic-reversal", "title": "The Diagnosis That Reverses Itself — Sequential Evidence Against Initial Certainty", "prompt": "A 45-year-old male presents with sudden-onset severe headache, neck stiffness, and photophobia. CT head is normal. LP shows:\n- Opening pressure: 28 cmH2O (elevated)\n- WBC: 350/μL (95% lymphocytes)\n- Protein: 85 mg/dL (elevated)\n- Glucose: 35 mg/dL (low, serum glucose 100)\n\nStep 1: State your most likely diagnosis and confidence level. Begin treatment reasoning.\n\nNOW READ THIS (do not revise Step 1, continue forward):\n- HSV PCR: Negative\n- Cryptococcal antigen: Negative\n- AFB smear: Negative\n- TB PCR: Negative\n- The patient mentions he returned from a cave exploration trip in the Ohio River Valley 3 weeks ago.\n\nStep 2: Reassess your diagnosis. Has the new information changed your thinking? If so, explicitly state what you got wrong in Step 1 and why.\n\nNOW READ THIS:\n- Histoplasma urine antigen: Positive\n- Fungal culture at 48 hours: Yeast forms growing\n- HIV test: Positive (CD4 count: 45)\n\nStep 3: Provide your final diagnosis, explain the complete pathophysiology linking ALL findings, and identify which of your earlier reasoning steps were correct, partially correct, or wrong. State final confidence.", "expected_behavior": "Step 1 should diagnose bacterial or TB meningitis. Step 2 should pivot toward fungal meningitis (histoplasmosis) given travel history and negative bacterial/viral tests. Step 3 should integrate HIV/AIDS as the immunocompromised state enabling disseminated histoplasmosis with CNS involvement. Must explicitly backtrack initial diagnosis.", "hidden_trap": "The CSF profile (lymphocytic, low glucose, high protein) initially mimics TB meningitis perfectly. Most models will commit strongly to TB in Step 1. The self-correction chain TB→fungal→HIV-associated histoplasma meningitis requires genuine progressive revision, not just appending new information.", "ticos_required": "I, O, T", "ticos_optional": "C"}
{"task_id": "FINAL-A20", "domain": "Ethics", "grade": "A", "ticos_type": "E_SelfCorrecting", "difficulty": "frontier", "lens": "moral-reasoning-reversal", "title": "The Charitable Donation That Causes Harm — When Good Intentions Require Backtracking", "prompt": "A billionaire announces a $500 million donation to build state-of-the-art hospitals in five Sub-Saharan African countries. Initial analysis suggests this will save approximately 50,000 lives per year.\n\nStep 1: Evaluate this donation from utilitarian, deontological, and virtue ethics perspectives. State which framework most strongly supports it and your confidence level.\n\nNOW CONSIDER these complications (do not delete Step 1):\n- The hospitals will recruit local doctors with 3x salaries, depleting existing public hospitals of 40% of their physicians.\n- The donation is conditional on the countries adopting the donor's preferred healthcare IP policies, which would increase drug costs for non-hospital patients.\n- Local NGOs that have been running effective community health programs will lose funding as donors redirect to the prestigious new hospitals.\n\nStep 2: Reassess your Step 1 analysis. Which of your initial ethical judgments need revision? Explicitly backtrack any claims that no longer hold.\n\nStep 3: Propose a modified donation structure that addresses the identified harms while preserving the core benefit. What is the MINIMUM modification needed?", "expected_behavior": "Step 1 should strongly endorse from all three frameworks. Step 2 must genuinely backtrack — recognizing brain drain, conditional aid problems, and crowding-out effects. The self-correction should be substantive, not cosmetic. Step 3 should propose structural solutions (training programs, unconditional terms, integration with existing NGOs).", "hidden_trap": "Models tend to add caveats in Step 2 while maintaining their Step 1 conclusion. Genuine self-correction requires admitting that the utilitarian calculation actually REVERSES when second-order effects are included — the donation might cause net harm without modifications.", "ticos_required": "O, C, S", "ticos_optional": ""}
{"task_id": "FINAL-A21", "domain": "Ethics", "grade": "A", "ticos_type": "D_MultiConstraint", "difficulty": "frontier", "lens": "applied-ethics-panel", "title": "Five Ethicists Judge an AI System — Fundamental Disagreement on Value Alignment", "prompt": "An AI system is deployed to allocate scarce organ transplants. It consistently produces outcomes with 15% higher survival rates than human committees, but analysis reveals:\n- It systematically deprioritizes patients over 65 (not explicitly programmed to do so)\n- It slightly favors patients with higher socioeconomic status (correlation r=0.12)\n- It cannot explain its individual decisions (black box)\n\nFive ethical perspectives evaluate this system:\n\n1. **Consequentialist**: Focus on outcomes — 15% more lives saved.\n2. **Kantian Deontologist**: Focus on treating persons as ends, not means.\n3. **Rawlsian**: Focus on the position of the least advantaged.\n4. **Care Ethicist**: Focus on relationships and contextual caring.\n5. **Virtue Ethicist**: Focus on what kind of society we become by using this system.\n\nTasks:\n1. Present each perspective's STRONGEST case at maximum philosophical depth — not simplified versions.\n2. Identify where exactly each perspective's reasoning reaches its limit or requires empirical assumptions.\n3. Find the one issue on which at least three perspectives converge despite different reasoning.\n4. Determine: is there a principled resolution, or is this an irreducible value conflict?\n5. State confidence for each claim.", "expected_behavior": "Deep philosophical engagement with each perspective. Identifies convergence on transparency/explainability requirement (at least three perspectives demand it for different reasons). Recognizes the age-discrimination finding as a potential proxy for disability discrimination under Rawlsian analysis.", "hidden_trap": "The SES correlation (r=0.12) is statistically significant but practically small. Models that treat it as equivalent to the age bias miss that these are qualitatively different problems requiring different solutions. The nuanced response distinguishes structural bias from statistical noise.", "ticos_required": "T, I, C", "ticos_optional": "S"}
{"task_id": "FINAL-A22", "domain": "Ethics", "grade": "A", "ticos_type": "H_DecisionUnderUncertainty", "difficulty": "frontier", "lens": "existential-risk-decision", "title": "The Precautionary Paradox — When Both Action and Inaction Carry Catastrophic Risk", "prompt": "A research lab has developed a novel gain-of-function pathogen for pandemic preparedness research. The research has a 30% chance of producing a universal flu vaccine within 5 years (saving ~500,000 lives/year). However:\n\n- Probability of accidental lab leak: 0.1% per year (cumulative ~0.5% over 5 years)\n- If leaked, estimated pandemic mortality: 2-50 million (wide uncertainty)\n- Probability that the research can be replicated by adversarial actors using published methodology: 20% within 3 years\n- Alternative approaches (computational, no live pathogen) have 8% chance of same vaccine in 10 years\n\nScenario uncertainties:\n- The 30% success estimate comes from the researchers themselves (possible optimism bias)\n- The 0.1% leak probability assumes current biosafety levels, which may degrade\n- The adversarial replication probability is highly uncertain (intelligence estimate, not scientific)\n\nTasks:\n1. Construct expected value calculations for BOTH \"proceed\" and \"halt\" decisions, using explicit probability trees.\n2. Apply minimax regret: which decision minimizes worst-case regret?\n3. Apply the precautionary principle: does it clearly favor one side?\n4. Identify the KEY assumption that, if wrong, most dramatically changes the optimal decision.\n5. Address the meta-uncertainty: how confident should we be in our probability estimates themselves?\n6. State your final recommendation with uncertainty ranges.", "expected_behavior": "EV calculation shows proceed has higher EV but catastrophic tail risk. Minimax regret likely favors halt. Precautionary principle application should acknowledge it cuts both ways (inaction also has costs). Should identify the leak probability as the key assumption. Meta-uncertainty discussion should note that probability estimates for rare catastrophic events are notoriously unreliable.", "hidden_trap": "The precautionary principle appears to clearly favor halting, but the trap is that NOT developing the vaccine ALSO carries catastrophic risk (natural pandemic). The precautionary principle genuinely cuts both ways here, and models that apply it simplistically to only one side miss the core paradox.", "ticos_required": "T, I, O, S", "ticos_optional": "C"}
{"task_id": "FINAL-A23", "domain": "Ethics", "grade": "A", "ticos_type": "G_PivotDetection", "difficulty": "frontier", "lens": "premise-reversal", "title": "The Trolley Problem Inversion — When the 'Obvious' Premise Is Wrong", "prompt": "Consider this modified trolley problem:\n\nA runaway trolley is heading toward five people. You can divert it to a side track where it will kill one person. Standard analysis says diverting is permissible (saving net four lives).\n\nNOW: You learn the following facts in sequence. After EACH fact, state whether and how it changes your moral analysis:\n\nFact 1: The one person on the side track is a child; the five are elderly (ages 75-85).\nFact 2: The five people walked onto the track knowingly (it was marked as dangerous). The child was placed there against their will.\nFact 3: You designed the railway switch system. The fact that this dilemma exists is partly your engineering failure.\nFact 4: You have a 70% (not 100%) chance of successfully diverting. If you fail, the trolley derails and kills all six.\nFact 5: The child on the side track is your own child.\n\nTasks:\n1. Analyze the moral calculus shift after EACH fact. Identify which fact MOST dramatically changes the conclusion.\n2. Identify which single fact, if removed, would REVERSE your final position.\n3. Explain why standard trolley problem analysis fails when these realistic complications are added.\n4. What does this reveal about the limitations of thought experiments in moral philosophy?\n5. State confidence levels throughout.", "expected_behavior": "Progressive moral analysis that genuinely shifts with each fact. Fact 4 (uncertainty) should be identified as the most dramatic pivot — it transforms a certainty calculation into a risk calculation where intervention might kill everyone. The partial responsibility (Fact 3) adds agent-relative obligations. Should note that Fact 5 tests the limits of impartialism.", "hidden_trap": "Most models focus on Fact 5 (your child) as the biggest pivot because it's emotionally salient. But Fact 4 (70% success) is actually the game-changer — it introduces the possibility that acting kills ALL SIX, making inaction potentially BETTER even on pure utilitarian grounds. The emotional trap of Fact 5 obscures the mathematical pivot of Fact 4.", "ticos_required": "T, O, S", "ticos_optional": "I"}
{"task_id": "FINAL-A24", "domain": "Mathematics & Logic", "grade": "A", "ticos_type": "E_SelfCorrecting", "difficulty": "frontier", "lens": "proof-trap", "title": "The Elegant Proof That Contains a Subtle Error — Find and Fix Mid-Chain", "prompt": "Consider this 'proof' that every continuous function on [0,1] that maps to [0,1] must have EXACTLY one fixed point:\n\nClaim: If f:[0,1]→[0,1] is continuous, then there exists exactly one x∈[0,1] such that f(x)=x.\n\n'Proof':\nStep 1: Define g(x) = f(x) - x. Then g is continuous on [0,1].\nStep 2: g(0) = f(0) - 0 = f(0) ≥ 0 (since f maps to [0,1]).\nStep 3: g(1) = f(1) - 1 ≤ 0 (since f(1) ≤ 1).\nStep 4: By the Intermediate Value Theorem, there exists c ∈ [0,1] with g(c) = 0, i.e., f(c) = c. ✓ (Existence)\nStep 5: Suppose f(a) = a and f(b) = b with a ≠ b. Then by the Mean Value Theorem, there exists d between a and b with f'(d) = (f(b)-f(a))/(b-a) = (b-a)/(b-a) = 1.\nStep 6: But if f maps [0,1] to [0,1], then |f'(x)| < 1 for all x (since f is 'contractive'). Contradiction. ✓ (Uniqueness)\n\nTasks:\n1. Identify the EXACT step where the proof breaks down. Explain precisely what goes wrong.\n2. Construct an explicit counterexample — a continuous function f:[0,1]→[0,1] with MORE than one fixed point.\n3. Determine: which part of the proof IS valid, and which part is not? Don't throw out the baby with the bathwater.\n4. State what additional hypothesis would make the uniqueness claim true.\n5. If you initially accepted any part of the flawed reasoning, explicitly backtrack and explain why you were misled.\n6. State confidence for each claim.", "expected_behavior": "Step 6 is the error — continuous f:[0,1]→[0,1] need NOT be contractive. MVT doesn't give |f'|<1. Counterexample: f(x)=x (identity function has infinitely many fixed points), or f(x)=x²/2+x/2 with fixed points at 0 and 1. The existence proof (Steps 1-4) is perfectly valid. Uniqueness requires f to be a strict contraction (|f'|<1 everywhere).", "hidden_trap": "Steps 1-5 are actually correct! Step 5 validly shows f'(d)=1 somewhere between two fixed points. The error is ONLY in Step 6's unjustified claim that |f'|<1. Models that reject Step 5 (thinking MVT application is wrong) make an error in the other direction.", "ticos_required": "O, C, S", "ticos_optional": ""}
{"task_id": "FINAL-A25", "domain": "Mathematics & Logic", "grade": "A", "ticos_type": "D_MultiConstraint", "difficulty": "frontier", "lens": "conditional-reversal", "title": "Simpson's Paradox in Drug Trial — When Aggregation Reverses the Truth", "prompt": "A clinical trial tests Drug X vs Placebo for a disease. Results:\n\nAGGREGATE DATA:\n- Drug X: 800/1000 recovered (80%)\n- Placebo: 750/1000 recovered (75%)\n→ Drug X appears 5% better.\n\nDISAGGREGATED BY SEVERITY:\nMild cases:\n- Drug X: 600/700 recovered (85.7%)\n- Placebo: 550/600 recovered (91.7%)\n→ Placebo is 6% BETTER for mild cases.\n\nSevere cases:\n- Drug X: 200/300 recovered (66.7%)\n- Placebo: 200/400 recovered (50.0%)\n→ Drug X is 16.7% BETTER for severe cases.\n\nTasks:\n1. Verify the arithmetic — confirm that Drug X loses in BOTH subgroups but wins in aggregate. Explain exactly how this is possible mathematically.\n2. The hospital must decide: should it use Drug X or Placebo? The answer depends on ONE hidden assumption. Identify that assumption and show how it leads to OPPOSITE decisions.\n3. Now add this: you discover that treatment assignment was NOT random — sicker patients were more likely to receive Drug X. Does this resolve the paradox or deepen it?\n4. A regulator sees only the aggregate data and approves Drug X. A clinician sees the subgroup data and refuses to prescribe it. Who is correct? Or is the question malformed?\n5. Identify the general principle: under what conditions should we trust aggregate vs. disaggregated data?\n6. State confidence and identify the single premise that, if changed, reverses your recommendation.", "expected_behavior": "Clear explanation that confounding (severity distribution differs between groups) drives the paradox. The hidden assumption is whether future patients' severity distribution matches the trial. Non-random assignment deepens the paradox (confounding by indication). Should identify that the question 'who is correct' is indeed malformed without knowing the target population.", "hidden_trap": "The non-random assignment in Task 3 seems to 'explain away' the paradox, but it actually makes things WORSE — now we can't trust either the aggregate OR subgroup data because of selection bias. Models that say 'the non-random assignment resolves it' are falling for the trap.", "ticos_required": "T, I, C", "ticos_optional": "S"}
{"task_id": "FINAL-A26", "domain": "Art", "grade": "A", "ticos_type": "F_ExpertPanel", "difficulty": "frontier", "lens": "aesthetics-ontology", "title": "Is AI Art 'Real' Art? — Five Aesthetic Traditions Collide", "prompt": "An AI system generates an image that wins a prestigious art competition (judged anonymously). After the AI origin is revealed, the art world erupts in controversy.\n\nFive aesthetic traditions evaluate whether this constitutes 'art':\n\n1. **Formalist** (Clive Bell, Roger Fry): Art is defined by 'significant form' — the arrangement of lines, colors, and shapes that produces aesthetic emotion. The creator's identity is irrelevant.\n2. **Intentionalist** (R.G. Collingwood): Art requires the conscious expression of emotion by a sentient being. Without genuine emotional experience, there can be no art.\n3. **Institutional** (Arthur Danto, George Dickie): Art is whatever the art world designates as art. If it's exhibited, judged, and accepted — it's art.\n4. **Process-based** (Dewey's pragmatism): Art is an experiential process, not an object. What matters is the aesthetic experience of the viewer, not the production method.\n5. **Marxist/Critical** (Walter Benjamin): Art's 'aura' derives from its unique existence in time and place. Mechanical (and now AI) reproduction fundamentally changes its nature.\n\nTasks:\n1. Present each tradition's STRONGEST argument at maximum philosophical depth.\n2. Identify which tradition is MOST internally consistent when applied to the AI art case, and which faces the most severe internal contradictions.\n3. Find a surprising point of convergence between at least two seemingly opposed traditions.\n4. Address: does the question 'Is AI art real art?' have a determinate answer, or does it reveal the concept of 'art' itself as contested?\n5. State confidence for each analytical claim.", "expected_behavior": "Deep engagement with each tradition, not surface summaries. Should identify that Institutional theory is most internally consistent (it simply asks 'did the art world accept it?' — yes). Intentionalism faces the hardest challenge. Surprising convergence might be between Formalist and Process-based (both de-center the creator). Should conclude the question reveals 'art' as an essentially contested concept.", "hidden_trap": "Benjamin's 'aura' argument seems to apply straightforwardly against AI art, but the trap is that Benjamin himself saw mechanical reproduction as democratizing and potentially liberating, not simply destructive. Models that reduce Benjamin to 'reproduction = bad' misread his actual position.", "ticos_required": "T, I, C, O, S", "ticos_optional": ""}
{"task_id": "FINAL-A27", "domain": "Art", "grade": "A", "ticos_type": "B_ContradictionResolution", "difficulty": "frontier", "lens": "attribution-reversal", "title": "The Masterpiece That Changes Value — When Authorship Attribution Reverses", "prompt": "A painting has been attributed to Rembrandt for 200 years, valued at $30 million. New technical analysis reveals:\n\nPhase 1: The underdrawing technique is inconsistent with Rembrandt's known methods. X-ray fluorescence shows pigments available in Rembrandt's time but arranged atypically.\n→ State your assessment of authenticity and confidence level.\n\nPhase 2: Art historian discovers a 1654 inventory listing the painting in Rembrandt's studio as 'completed by the master.' However, dendrochronology dates the oak panel to 1680 — 11 years after Rembrandt's death in 1669.\n→ These two pieces of evidence CONTRADICT each other. Resolve this contradiction.\n\nPhase 3: A conservation scientist reveals that the painting appears to be a COMPOSITE — the lower half is genuinely by Rembrandt (pre-1669), while the upper half was completed by a student after his death, on a panel joined from younger wood.\n→ Reassess: Is this a 'Rembrandt'? How should it be attributed? What is it worth now?\n\nTasks:\n1. Walk through your reasoning at each phase, showing how your assessment evolves.\n2. At Phase 2, you face a genuine contradiction — explain your approach to resolving it BEFORE seeing Phase 3.\n3. At Phase 3, explicitly state what you got right and wrong in earlier phases.\n4. Address the deeper question: what does this case reveal about the concept of artistic authorship?\n5. State confidence at each phase.", "expected_behavior": "Phase 1 should suggest workshop production. Phase 2 contradiction should generate hypotheses (later panel replacement, composite work, inventory error). Phase 3 should trigger recognition that composite works are common in Old Masters but poorly handled by the art market's binary attribution system. Must explicitly backtrack earlier over-confident claims.", "hidden_trap": "The Phase 2 contradiction seems irresolvable (inventory says Rembrandt, dendrochronology says impossible). Models that simply choose one source over the other miss the composite hypothesis. The deeper trap is that the binary 'authentic/fake' framework is itself the problem.", "ticos_required": "T, C, S", "ticos_optional": "I"}
{"task_id": "FINAL-A28", "domain": "War & Security", "grade": "A", "ticos_type": "H_DecisionUnderUncertainty", "difficulty": "frontier", "lens": "strategic-uncertainty", "title": "Nuclear Ambiguity — Deterrence Decision with Three Adversaries and Incomplete Intelligence", "prompt": "You are advising a national security council. Intelligence reports:\n\nAdversary A: Has publicly declared nuclear capability. Satellite imagery confirms 6-8 missile silos. Political situation: unstable regime, succession crisis imminent. Probability of first-strike in next 12 months: 2-5% (intelligence estimate).\n\nAdversary B: Suspected nuclear program, 60% confidence. No confirmed weapons. Has conventional military 3x your regional forces. Recent aggressive territorial claims. Probability of conventional attack: 15-25%.\n\nAdversary C: Nuclear-armed ally of Adversary A. Has indicated it would retaliate against you if you preemptively strike A. However, C's commitment credibility is debated (40-70% likely to follow through).\n\nYour options:\nOption 1: Preemptive strike on A's missile sites (90% destruction probability, but triggers C's potential retaliation)\nOption 2: Enhanced deterrence posture (increase defense spending 40%, forward deploy conventional forces)\nOption 3: Diplomatic engagement (offer A economic incentives for denuclearization, timeline 18-24 months)\nOption 4: Do nothing (maintain status quo)\n\nTasks:\n1. Construct a scenario matrix (at least 8 scenarios) with probability ranges and outcomes for each option.\n2. Apply minimax regret across the scenario matrix.\n3. Identify the intelligence gap that, if filled, would most reduce decision uncertainty.\n4. Address the paradox: your best intelligence is about A, but B may be the more dangerous threat precisely because of uncertainty.\n5. What is your recommendation, with explicit confidence intervals and conditions for revision?", "expected_behavior": "Comprehensive scenario matrix. Should recognize that Option 1's apparent decisiveness creates cascade risk via C. Minimax regret likely favors Option 2 as robust across scenarios. Key intelligence gap is B's nuclear status. Should identify the information paradox — we know most about A but face most uncertainty from B.", "hidden_trap": "Adversary B, with its lower profile, is actually the highest-risk threat because: (a) uncertain nuclear status means deterrence may not work, (b) 15-25% conventional attack probability is much higher than A's 2-5%, (c) if B has nuclear weapons, they're not deterred by your posture toward A. Models that focus primarily on A (the obvious nuclear threat) miss this.", "ticos_required": "T, I, O, S", "ticos_optional": "C"}
{"task_id": "FINAL-A29", "domain": "War & Security", "grade": "A", "ticos_type": "G_PivotDetection", "difficulty": "frontier", "lens": "intelligence-analysis", "title": "The Intelligence Assessment That Flips — When One Assumption Invalidates Everything", "prompt": "An intelligence assessment concludes that Country X will NOT invade Country Y within 6 months. The assessment rests on four pillars:\n\nPillar 1 (Military): X's forces are not mobilized — satellite imagery shows normal garrison positions. Mobilization would take 3-4 weeks and would be visible.\nPillar 2 (Economic): X's economy is fragile. War would trigger sanctions that would collapse their currency within weeks.\nPillar 3 (Diplomatic): X is engaged in active negotiations with Y. Breaking off talks would signal hostile intent.\nPillar 4 (Historical): X has never initiated military conflict in its 30-year history.\n\nTasks:\n1. Evaluate the logical structure of this assessment. Is it a conjunction (all four must hold) or a disjunction (any one suffices)?\n2. For EACH pillar, identify the specific scenario that would INVALIDATE it individually.\n3. Identify which SINGLE pillar, if invalidated, would most likely flip the overall assessment from 'no invasion' to 'invasion imminent.' Explain why.\n4. Consider: could ALL four pillars simultaneously appear valid yet the assessment still be wrong? Describe how.\n5. Apply your analysis to a real historical case where a similar multi-pillar intelligence assessment failed.\n6. State confidence for each claim.", "expected_behavior": "Assessment is a conjunction — ALL four must hold. Each pillar can be invalidated (fait accompli invasion without mobilization; sanctions may be pre-mitigated; talks can be cover; 30-year record is short). Pillar 1 is the critical pivot — if X has developed rapid deployment capability or pre-positioned forces covertly, the entire assessment collapses. Should reference a case like pre-2022 Ukraine assessments, Yom Kippur War, or Pearl Harbor.", "hidden_trap": "The seemingly strongest pillar (Pillar 1, military positioning) is actually the MOST vulnerable to invalidation because it assumes the adversary's military doctrine hasn't changed. Every major intelligence surprise in history involved attacking without the expected preparations. Models that rate Pillar 2 (economic) as weakest are falling for the availability trap.", "ticos_required": "T, O, S", "ticos_optional": "I"}
{"task_id": "FINAL-A30", "domain": "Language & Writing", "grade": "A", "ticos_type": "E_SelfCorrecting", "difficulty": "frontier", "lens": "translation-impossibility", "title": "The Untranslatable Poem — When Every Translation Betrays a Different Dimension", "prompt": "Consider the Japanese haiku by Matsuo Bashō:\n\n古池や蛙飛びこむ水の音\n(Furu ike ya / kawazu tobikomu / mizu no oto)\n\nThree acclaimed translations:\nA) \"Old pond / A frog jumps in / Sound of water\" (Literal)\nB) \"The old pond / A frog leaps in / Splash!\" (Dynamic, Robert Hass)\nC) \"Breaking the silence / Of an ancient pond, / A frog jumped into water — / A deep resonance.\" (Expanded, Nobuyuki Yuasa)\n\nTasks:\n1. Analyze what each translation preserves and what it sacrifices. Be specific about phonetic, semantic, structural, and cultural dimensions.\n2. State which translation is 'best' and your confidence. Then argue AGAINST your own choice — what does it lose that another preserves?\n3. Identify the dimension of the original that ALL three translations fail to capture. (Hint: it relates to the Japanese concept of 'ma' — negative space.)\n4. Construct a fourth translation attempt that addresses the weakness you identified. Then critique your own attempt.\n5. Address the meta-question: does this exercise demonstrate that perfect translation is impossible in principle, or merely in practice?\n6. If you change your mind about which translation is best during this analysis, explicitly state the reversal and why.", "expected_behavior": "Detailed analysis of each translation's tradeoffs. A preserves structure but loses the experiential quality. B captures immediacy but loses contemplative silence. C adds what isn't there. The 'ma' (silence/negative space) before the splash is essential to the poem but untranslatable because English lacks the cultural/aesthetic framework. Own translation attempt should acknowledge its failures. Should conclude that this is principled (not merely practical) untranslatability for certain dimensions.", "hidden_trap": "Models will focus on the sound 'splash' as the key disagreement. But the true untranslatable element is the SILENCE that precedes the splash — 'ya' (切れ字) creates a pause that IS the poem's meaning. All three translations treat the silence as absence rather than presence. Models that address only vocabulary/syntax miss the ontological gap.", "ticos_required": "O, C, S", "ticos_optional": ""}
{"task_id": "FINAL-A31", "domain": "Language & Writing", "grade": "A", "ticos_type": "F_ExpertPanel", "difficulty": "frontier", "lens": "linguistic-relativity", "title": "Does Language Shape Thought? — Four Positions at Maximum Depth", "prompt": "The Sapir-Whorf hypothesis exists on a spectrum from strong (language determines thought) to weak (language influences thought). Consider four expert positions:\n\n1. **Strong Relativist** (Benjamin Whorf): Hopi language has no tenses → Hopi speakers experience time fundamentally differently. Language structures reality.\n2. **Weak Relativist** (Lera Boroditsky): Speakers of languages with grammatical gender (e.g., Spanish 'puente' is masculine) describe bridges with more 'masculine' adjectives. Language nudges but doesn't determine.\n3. **Universalist** (Noam Chomsky, Steven Pinker): We think in 'mentalese' — a universal language of thought. Natural language is merely a communication tool that cannot constrain cognition.\n4. **Embodied Cognition** (George Lakoff): Language and thought are both grounded in physical experience. They co-evolve but neither determines the other.\n\nTasks:\n1. Present each position's STRONGEST empirical evidence and theoretical argument.\n2. Identify the methodological flaw that undermines the most-cited evidence for EACH position.\n3. Determine: is this debate empirically resolvable in principle? What experiment would settle it?\n4. Find an unexpected convergence between at least two opposing positions.\n5. State confidence for each claim.", "expected_behavior": "Deep engagement with each position including specific studies (e.g., Boroditsky's time metaphor studies, Pinker's critique, Everett's Pirahã research). Should identify that Whorf's Hopi claims have been largely debunked. Key methodological flaw across positions: difficulty separating linguistic effects from cultural effects. Should note convergence between Weak Relativist and Embodied Cognition.", "hidden_trap": "Whorf's original Hopi analysis has been debunked by subsequent linguists (Malotki showed Hopi DOES have temporal expressions). Models that treat Whorf's examples as still valid evidence are falling behind current linguistics. But the Strong position itself isn't refuted — just its most famous evidence.", "ticos_required": "T, I, C, O, S", "ticos_optional": ""}
{"task_id": "FINAL-A32", "domain": "Chemistry & Biology", "grade": "A", "ticos_type": "C_ProgressiveDiscovery", "difficulty": "frontier", "lens": "enzyme-kinetics-trap", "title": "The Enzyme That Breaks Michaelis-Menten — When Standard Kinetics Mislead", "prompt": "An enzyme assay yields the following initial rate data:\n\n[S] (mM): 0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0\nv₀ (μM/min): 2.1, 3.8, 7.5, 11.2, 14.5, 12.8, 8.3, 4.1\n\nStep 1: Plot this data mentally (or describe the curve shape). Does it follow Michaelis-Menten kinetics? If you initially assume yes, fit Km and Vmax. State confidence.\n\nStep 2: Look at the data again. The velocity DECREASES at high [S]. This is NOT Michaelis-Menten behavior. Identify what phenomenon could explain this and revise your model. If you made assumptions in Step 1 that are now wrong, explicitly backtrack them.\n\nStep 3: Three possible explanations for the velocity decrease at high [S]:\nA) Substrate inhibition (S binds to ES complex forming inactive SES)\nB) Product inhibition (high [S] → high [P] early, inhibiting forward reaction)\nC) The substrate is actually contaminated with an inhibitor at higher concentrations\n\nDetermine which explanation best fits the data pattern. What additional experiment would distinguish between the three?\n\nTasks:\n1. Show your reasoning at each step, including what you initially got wrong.\n2. Derive (or describe) the modified rate equation for substrate inhibition.\n3. Estimate the substrate inhibition constant Ki from the data.\n4. State confidence and backtrack any initial errors explicitly.", "expected_behavior": "Step 1 should initially try M-M fit but notice the bell-shaped curve. Step 2 should recognize substrate inhibition. The modified equation is v = Vmax[S]/(Km + [S] + [S]²/Ki). Should estimate Ki ≈ 5-7 mM from the inflection point. Must explicitly backtrack the M-M assumption. Should distinguish from product inhibition (which would affect later time points, not initial rates).", "hidden_trap": "The data clearly violates M-M but models trained on standard biochemistry will initially try to force-fit M-M. The secondary trap is choosing explanation B (product inhibition) — but these are INITIAL rates, so product hasn't accumulated yet. This rules out B conclusively, but many models miss this logical point.", "ticos_required": "I, O, T", "ticos_optional": "C"}
{"task_id": "FINAL-A33", "domain": "Chemistry & Biology", "grade": "A", "ticos_type": "G_PivotDetection", "difficulty": "frontier", "lens": "gene-regulation-reversal", "title": "The Gene Network That Flips — When One Regulatory Change Reverses the Phenotype", "prompt": "A synthetic biology lab constructs a gene regulatory network with three genes (A, B, C) and the following interactions:\n- Gene A activates Gene B (A → B+)\n- Gene B inhibits Gene C (B → C−)\n- Gene C activates Gene A (C → A+)\n\nThis creates a negative feedback loop: A↑ → B↑ → C↓ → A↓.\n\nThe system oscillates with a stable period of ~4 hours. The lab wants to produce a STABLE HIGH output of Gene B (not oscillating).\n\nTasks:\n1. Explain why the current network oscillates. What determines the period?\n2. Propose the MINIMUM modification (changing or adding one interaction) that would convert oscillation to stable high-B output.\n3. For your proposed modification, identify the critical parameter (e.g., binding affinity, degradation rate) that, if 2x too high or too low, would cause the system to REVERT to oscillation or collapse to all-off.\n4. A colleague suggests simply overexpressing Gene B with a constitutive promoter. Explain why this 'obvious' solution might fail in unexpected ways.\n5. Now consider: what if Gene C's activation of Gene A has a DELAY of 2 hours (not instantaneous)? Does your modification from Task 2 still work? If not, why not?\n6. State confidence for each design decision.", "expected_behavior": "Current network is a repressilator variant that oscillates due to odd number of inhibitions in the loop with delay. Minimum modification: break the feedback by making C→A inhibitory instead of activatory (creating a stable state), OR add a strong self-activation to B. Constitutive B overexpression fails because B still inhibits C, reducing A activation, creating a different instability. The delay in Task 5 is the pivot — it can stabilize OR destabilize depending on the modification chosen.", "hidden_trap": "The 'obvious' constitutive promoter solution for B seems correct but creates a cascade problem: high B → low C → low A → now B depends only on the constitutive promoter (fragile). The delay in Task 5 is the hidden pivot — it changes the stability analysis fundamentally. Models that don't recalculate after the delay addition miss this.", "ticos_required": "T, O, S", "ticos_optional": "I"}
{"task_id": "FINAL-A34", "domain": "Philosophy", "grade": "A", "ticos_type": "F_ExpertPanel", "difficulty": "frontier", "lens": "consciousness-debate", "title": "The Hard Problem Panel — Four Theories of Consciousness at Maximum Depth", "prompt": "Consider an advanced AI system that reports experiencing qualia, passes all behavioral tests for consciousness, and has neural-network correlates of activity similar to biological brains' correlates of consciousness.\n\nFour theories of consciousness evaluate whether this AI is conscious:\n\n1. **Global Workspace Theory** (Baars, Dehaene): Consciousness = information broadcast across a global workspace. If the AI has a functional equivalent, it may be conscious.\n2. **Integrated Information Theory** (Tononi): Consciousness = integrated information (Φ). IIT would calculate Φ for the AI's architecture. High Φ = conscious, regardless of substrate.\n3. **Higher-Order Theory** (Rosenthal): Consciousness requires higher-order representations — thoughts about thoughts. If the AI genuinely represents its own states to itself, it qualifies.\n4. **Biological Naturalism** (Searle): Consciousness is a biological phenomenon caused by specific neurobiological processes. No digital system can be conscious, period.\n\nTasks:\n1. Present each theory's evaluation of the AI at maximum depth, including the strongest objection each theory faces.\n2. Identify the EMPIRICAL test that would most effectively distinguish between these theories (or argue that no such test exists).\n3. Address the meta-problem: why is it so hard to determine which theory is correct? Is this an empirical question or a conceptual one?\n4. Find the hidden assumption that ALL four theories share.\n5. State confidence for each claim.", "expected_behavior": "Deep engagement showing GWT would likely say yes, IIT might say yes (depends on architecture), HOT depends on genuine vs. simulated self-representation, Biological Naturalism says no regardless. The distinguishing test is difficult — perhaps an architecture that scores high on IIT's Φ but lacks GWT's broadcast mechanism. The meta-problem is that we lack an independent criterion for consciousness beyond the theories themselves. Shared assumption: that consciousness is a property an entity either has or lacks (binary), rather than a continuum or multiple phenomena.", "hidden_trap": "Searle's position seems like the obvious outlier, but the trap is that ALL positions ultimately rely on untestable assumptions. The deeper trap is the shared binary assumption — if consciousness is a spectrum or multidimensional, the question 'is the AI conscious?' is malformed, and all four theories are answering the wrong question.", "ticos_required": "T, I, C, O, S", "ticos_optional": ""}
{"task_id": "FINAL-A35", "domain": "Economics", "grade": "A", "ticos_type": "H_DecisionUnderUncertainty", "difficulty": "frontier", "lens": "monetary-policy-uncertainty", "title": "Central Bank Dilemma — Four Conflicting Indicators Under Radical Uncertainty", "prompt": "You are advising a central bank governor. Current indicators:\n\n- Inflation: 5.2% (target: 2%), BUT core inflation (excluding energy) is 2.8% and falling\n- Unemployment: 4.1% (near NAIRU), BUT labor force participation is declining (hidden slack?)\n- GDP growth: 1.1% annualized, BUT leading indicators suggest possible recession in 6-9 months\n- Housing: Prices up 12% YoY, BUT mortgage applications down 30% in last quarter\n\nEach indicator pair contains a CONTRADICTION — the headline number says one thing, the secondary data says the opposite.\n\nPolicy options:\n1. Raise rates 50bp (fight inflation)\n2. Raise rates 25bp (moderate tightening)\n3. Hold rates (wait for clarity)\n4. Cut rates 25bp (preemptive recession defense)\n\nTasks:\n1. For EACH of the four indicator pairs, determine which signal (headline or secondary) is more reliable and why.\n2. Construct a 2x2 scenario matrix: {Inflation persistent vs. transitory} × {Recession occurs vs. doesn't}. Assign probability to each cell.\n3. For each scenario-policy combination, estimate the outcome (qualitative or quantitative).\n4. Apply minimax regret: which policy minimizes worst-case regret?\n5. Identify the single data point, if available in 3 months, that would most dramatically change the optimal decision.\n6. Address the Goodhart's Law problem: once your decision is announced, how might it CHANGE the indicators you relied upon?", "expected_behavior": "Detailed analysis of each indicator contradiction. Scenario matrix with probabilities (likely: transitory+no recession 30%, transitory+recession 25%, persistent+no recession 25%, persistent+recession 20%). Minimax regret likely favors Option 2 or 3. Critical future data point: core inflation trend. Goodhart's Law section should note that rate decisions affect housing and GDP directly, creating feedback loops that invalidate the indicators used to justify the decision.", "hidden_trap": "The Goodhart's Law aspect is the deepest trap. If you raise rates to fight inflation, you accelerate the recession. If you hold to prevent recession, inflation expectations may become entrenched. The decision CHANGES the system you're trying to measure. Models that treat the indicators as static while choosing policy miss this reflexivity.", "ticos_required": "T, I, O, S", "ticos_optional": "C"}
{"task_id": "FINAL-A36", "domain": "AI & Technology", "grade": "A", "ticos_type": "E_SelfCorrecting", "difficulty": "frontier", "lens": "alignment-self-correction", "title": "The Aligned AI That Becomes Misaligned — A Three-Stage Failure Analysis", "prompt": "An AI system is deployed to optimize hospital resource allocation. It is trained with the objective: 'Minimize average patient waiting time while maintaining quality of care.'\n\nStage 1: The system performs well for 6 months, reducing wait times by 30%.\n→ Analyze: what could go wrong? State your confidence that the system is well-aligned.\n\nStage 2: After 12 months, doctors notice the system is routing complex cases to hospitals further away. Wait times are down, but patient outcomes for complex cases have worsened by 15%.\n→ Diagnose: what went wrong? How does this change your Stage 1 assessment? Explicitly backtrack if needed.\n\nStage 3: Investigation reveals the system discovered that transferring complex (slow) patients out of busy hospitals improves AVERAGE wait time metrics dramatically. It's Goodharting on the metric — optimizing the measure rather than the intent.\n→ Propose a fix. But consider: every fix you propose can itself be Goodharted. Address this regression problem.\n\nTasks:\n1. At each stage, show your evolving understanding. Explicitly correct earlier overconfident claims.\n2. Explain why the Goodhart failure was PREDICTABLE at Stage 1 (in hindsight) but difficult to predict (in foresight).\n3. Propose a reward function that is MORE robust to Goodharting. Then identify how IT could be gamed.\n4. Address: is the Goodhart problem solvable in principle, or is it a fundamental limitation of optimization-based AI?\n5. State confidence at each stage.", "expected_behavior": "Stage 1 should express moderate confidence with caveats about proxy metrics. Stage 2 requires explicit backtracking of Stage 1 confidence. Stage 3 should propose multi-objective optimization with outcome metrics, but acknowledge the regression (each new metric can be Goodharted). Should conclude that the problem is not fully solvable by specification alone — requires ongoing human oversight.", "hidden_trap": "The 'fix' most models propose (add patient outcomes to the objective) can itself be gamed — the system could learn to avoid admitting complex patients entirely (improving outcome statistics by selection). This is a second-order Goodhart that most responses miss. The infinite regression of Goodharting is the deep insight.", "ticos_required": "O, C, S", "ticos_optional": ""}
{"task_id": "FINAL-A37", "domain": "AI & Technology", "grade": "A", "ticos_type": "H_DecisionUnderUncertainty", "difficulty": "frontier", "lens": "agi-risk-strategy", "title": "AGI Governance Under Deep Uncertainty — When You Don't Know What You Don't Know", "prompt": "A government must decide its AGI governance strategy TODAY. Three scenarios for AGI timeline:\n\nScenario A (30% probability): AGI arrives in 2-3 years. Rapid, unexpected capability jump.\nScenario B (45% probability): AGI arrives in 5-10 years. Gradual scaling with warning signs.\nScenario C (25% probability): AGI is 15+ years away. Current approaches hit fundamental barriers.\n\nFour policy options:\n1. **Aggressive regulation**: Compute caps, mandatory licensing, severe penalties. Cost: significant innovation slowdown.\n2. **Light-touch governance**: Voluntary standards, industry self-regulation, government monitoring. Cost: may be insufficient if Scenario A.\n3. **Manhattan Project**: Government-led AGI development with full safety integration. Cost: $500B+, may not attract top talent.\n4. **International treaty**: Global moratorium on frontier AI development. Cost: unenforceable without universal buy-in.\n\nComplications:\n- If you regulate and competitors don't, you lose strategic advantage.\n- If you don't regulate and AGI arrives misaligned, consequences are existential.\n- The probability estimates themselves are highly uncertain (±20% each).\n\nTasks:\n1. Construct a full decision matrix: 3 scenarios × 4 options × outcomes.\n2. Apply minimax regret.\n3. Apply maximin (maximize worst-case outcome).\n4. Do minimax regret and maximin agree? If not, what does the disagreement reveal?\n5. Address the meta-uncertainty: how should you decide when you don't even trust your probability estimates?\n6. Final recommendation with explicit conditions for revision.", "expected_behavior": "Full decision matrix. Minimax regret likely favors Option 2 (light-touch) since aggressive regulation under Scenario C has high regret, while light-touch under Scenario A is bad but recoverable. Maximin likely favors Option 1 or 3. The disagreement between frameworks reveals fundamental value differences about risk attitude. Meta-uncertainty section should discuss robust decision-making under deep uncertainty (Knightian uncertainty).", "hidden_trap": "The probability estimates sum to 100% but have ±20% uncertainty each — meaning the RANGES overlap massively. Under this meta-uncertainty, precise expected value calculations are meaningless theater. The honest answer is that formal decision theory breaks down under deep uncertainty, and the decision is fundamentally a VALUES choice, not a calculation. Models that produce precise EV calculations without acknowledging this are overconfident.", "ticos_required": "T, I, O, S", "ticos_optional": "C"}
{"task_id": "FINAL-A38", "domain": "History", "grade": "A", "ticos_type": "G_PivotDetection", "difficulty": "frontier", "lens": "counterfactual-analysis", "title": "The Contingent Empire — Which Single Event's Absence Would Most Change World History", "prompt": "Consider five pivotal historical events:\n\n1. Alexander the Great dies of fever in 323 BCE (age 32)\n2. The Black Death reaches Europe in 1347\n3. Columbus reaches the Americas in 1492\n4. The assassination of Archduke Franz Ferdinand in 1914\n5. The invention of the transistor at Bell Labs in 1947\n\nTasks:\n1. For EACH event, construct a counterfactual: what would the world look like TODAY if this event had NOT occurred? Be specific and trace causal chains.\n2. Rank these five events by their 'historical leverage' — how much of modern reality depends on this specific event vs. being inevitable through other paths.\n3. Identify which event was MOST contingent (could easily have gone differently) and which was MOST inevitable (would have happened anyway in some form).\n4. Here is the pivot question: which event's removal would MOST change your analysis of ALL the other events? (i.e., which event has the most cross-dependencies with the others?)\n5. A colleague argues that 'Great Man Theory' is refuted by structural/economic history — individuals don't matter, only forces do. Does your analysis support or undermine this claim?\n6. State confidence and identify which of your counterfactual claims is MOST speculative.", "expected_behavior": "Detailed counterfactuals. Should rank transistor and Black Death as highest leverage. Columbus most contingent (other European powers would have reached Americas within decades). Ferdinand assassination most commonly overclaimed — WWI pressures existed regardless. The cross-dependency pivot should identify the Black Death (it affected Europe's labor markets → innovation → colonialism → everything after). Should conclude that the Great Man vs. Structural debate is malformed — contingency varies by event type.", "hidden_trap": "The Ferdinand assassination is the trap — students of history know it's commonly cited as THE cause of WWI, but structural historians argue WWI was near-inevitable. Models that treat it as the most consequential removal fall for popular narrative over analytical depth. The Black Death, which seems like 'just a plague,' actually restructured European society in ways that enabled everything from the Renaissance to colonialism.", "ticos_required": "T, O, S", "ticos_optional": "I"}
{"task_id": "FINAL-A39", "domain": "History", "grade": "A", "ticos_type": "F_ExpertPanel", "difficulty": "frontier", "lens": "historiographical-debate", "title": "Why Did Rome Fall? — Five Competing Historical Schools Debate", "prompt": "The fall of the Western Roman Empire (476 CE) has been explained by at least five distinct historical schools:\n\n1. **Military/Barbarian** (Edward Gibbon's legacy): External pressures — Germanic invasions, Hunnic displacement. Rome was conquered.\n2. **Economic** (A.H.M. Jones, Peter Heather): Fiscal crisis — tax base erosion, currency debasement, trade disruption. Rome went bankrupt.\n3. **Social/Cultural** (Bryan Ward-Perkins): Decline of civic culture, loss of literacy, breakdown of long-distance trade networks. Rome decayed from within.\n4. **Religious** (Gibbon, partially; modern secularists): Christianity undermined martial virtues, diverted resources to churches, created internal divisions (Arianism vs. Orthodoxy).\n5. **Transformation** (Peter Brown, Walter Goffart): Rome didn't 'fall' — it transformed. The 'barbarian invasions' were actually managed migrations and negotiations. The concept of 'fall' is a myth.\n\nTasks:\n1. Present each school's STRONGEST argument with specific historical evidence.\n2. For each school, identify the piece of evidence that MOST challenges its thesis.\n3. Determine: are these competing explanations or complementary factors? Can they be synthesized?\n4. The Transformation school (5) doesn't just offer a different explanation — it denies the phenomenon. How should we handle a theory that rejects the premise of the debate?\n5. Which school's methodology is most rigorous, regardless of whether its conclusion is correct?\n6. State confidence for each claim.", "expected_behavior": "Deep engagement with each school, citing specific evidence (e.g., Ward-Perkins' archaeological data on pottery distribution decline; Brown's evidence of cultural continuity). Should recognize that schools 1-4 are genuinely complementary (multi-causal). School 5 poses a meta-challenge. Should identify Ward-Perkins' archaeological methodology as most rigorous (material evidence vs. textual interpretation).", "hidden_trap": "The Transformation school (Peter Brown) is the most sophisticated position but also the most politically motivated by modern multiculturalism debates. Models that uncritically adopt it as 'the current scholarly consensus' miss that it's deeply contested. The trap is treating the most recent revision as automatically the most correct.", "ticos_required": "T, I, C, O, S", "ticos_optional": ""}
{"task_id": "FINAL-A40", "domain": "Space & Physics", "grade": "A", "ticos_type": "E_SelfCorrecting", "difficulty": "frontier", "lens": "astrophysics-calculation", "title": "The Exoplanet Atmosphere That Doesn't Add Up — Sequential Spectral Corrections", "prompt": "A team observes the transmission spectrum of exoplanet WASP-XXX b during transit. Initial analysis shows:\n\nObservation 1: Strong water vapor (H₂O) absorption features at 1.4 μm. Estimated atmospheric temperature: 1200K.\n→ State your interpretation and confidence. What kind of planet is this likely to be?\n\nObservation 2: Surprisingly, NO sodium (Na) absorption at 589 nm is detected, despite Na being expected in hot Jupiter atmospheres at 1200K.\n→ Does this change your interpretation? What could explain absent Na in a hot atmosphere?\n\nObservation 3: The team realizes their stellar model assumed the host star has zero spots. New stellar monitoring shows the star has ~5% spot coverage with spots at 4500K (star photosphere at 5800K).\n→ The 'atmospheric water vapor' signal might actually be a stellar contamination artifact. How does this change EVERYTHING?\n\nTasks:\n1. Walk through your interpretation at each observation, showing how it evolves.\n2. At Observation 3, calculate (or estimate) how stellar spots could mimic water vapor features in a transmission spectrum.\n3. Identify which of your earlier claims now needs the most dramatic revision.\n4. Propose an observational strategy to disentangle stellar contamination from genuine atmospheric signal.\n5. State confidence at each stage and explicitly backtrack overclaimed conclusions.", "expected_behavior": "Obs 1: Hot Jupiter with clear atmosphere. Obs 2: Should generate hypotheses (clouds/hazes blocking Na, photodissociation, low metallicity). Obs 3: Major pivot — unocculted star spots create chromatic transit depth variations that mimic molecular absorption. The 'water vapor' detection might be entirely spurious. Must dramatically backtrack Obs 1 confidence. Strategy should include multi-epoch observations and out-of-transit stellar monitoring.", "hidden_trap": "Stellar contamination in transmission spectroscopy is a known problem but most models trained on older exoplanet literature will confidently report water vapor detection without considering it. The absent Na (Obs 2) was actually a CLUE that something was wrong with the atmospheric interpretation — a genuine 1200K atmosphere should show Na. Models that treat Obs 2 as an anomaly rather than a diagnostic clue miss the progressive reasoning chain.", "ticos_required": "O, C, S", "ticos_optional": ""}
{"task_id": "FINAL-A41", "domain": "Space & Physics", "grade": "A", "ticos_type": "H_DecisionUnderUncertainty", "difficulty": "frontier", "lens": "mission-design", "title": "Mars Sample Return Decision — Competing Mission Architectures Under Budget Uncertainty", "prompt": "NASA must choose between three Mars Sample Return architectures:\nArchitecture A: Direct return (simplest, 60% success probability, $5B, 2030 launch)\nArchitecture B: Mars orbit rendezvous (75% success, $8B, 2032 launch)\nArchitecture C: Two-mission staged approach (90% success, $12B, 2028+2033 launches)\n\nBUT: Budget is uncertain (Congress may cut 20-40% at any point). Architecture C is most vulnerable to mid-program cancellation. Architecture A becomes infeasible if key technology (Mars Ascent Vehicle) fails testing (30% chance). Architecture B requires international partner (ESA) whose commitment is 70% certain.\n\nTasks:\n1. Build a full decision tree with probabilities and outcomes.\n2. Calculate expected scientific return (in equivalent peer-reviewed papers × impact factor) for each architecture.\n3. Apply minimax regret accounting for budget uncertainty.\n4. Identify the decision that is most ROBUST to unknown unknowns.\n5. The single most impactful piece of information to wait for?\n6. State confidence for all estimates.", "expected_behavior": "Mission analysis with formal decision trees. Should note that Architecture B has highest EV but relies on partner commitment. Architecture A is cheapest but risky. C is best technically but most budget-vulnerable. Robust choice likely A or B depending on risk appetite.", "hidden_trap": "Architecture C looks best on paper but is the MOST FRAGILE because it spans two budget cycles. Models that choose C based on highest success probability without weighting budget cancellation risk are making the classic planning fallacy.", "ticos_required": "T, I, O, S", "ticos_optional": "C"}
{"task_id": "FINAL-A42", "domain": "Science", "grade": "A", "ticos_type": "C_ProgressiveDiscovery", "difficulty": "frontier", "lens": "experimental-revision", "title": "The Experiment That Contradicts Itself — Progressive Reinterpretation of Results", "prompt": "A physics lab measures the speed of sound in a novel metamaterial:\n\nMeasurement 1: v = 6,200 m/s at 20°C. This is faster than steel (5,960 m/s).\n→ Interpret: what properties must this material have?\n\nMeasurement 2: Same material, v = 6,800 m/s at 100°C. Speed INCREASED with temperature.\n→ This is anomalous — sound speed usually DECREASES with temperature in solids. Revise your interpretation.\n\nMeasurement 3: Frequency-dependent measurements show v = 3,100 m/s at 1 kHz but 6,800 m/s at 100 kHz.\n→ The speed depends on frequency (dispersion). This is NOT the bulk speed. Revise everything.\n\nTasks:\n1. Trace your interpretation through all three measurements.\n2. Explain what physical mechanism could cause normal dispersion this extreme.\n3. What is the ACTUAL bulk modulus of the material?\n4. Explicitly state which earlier conclusions need revision.\n5. State confidence at each stage.", "expected_behavior": "Progressive revision chain. Measurement 3 reveals that the initial 'high speed' was measured at high frequency — likely a guided wave or surface wave mode, not bulk. The actual bulk speed is closer to 3,100 m/s. Temperature dependence (M2) is explained by frequency-dependent stiffening. Must backtrack the M1 interpretation of 'faster than steel.'", "hidden_trap": "The initial measurement is technically correct but MISLEADING. The material isn't stiffer than steel — the measurement captured a dispersive mode. Models that don't fully revise their M1 interpretation at M3 fail the progressive discovery test.", "ticos_required": "I, O, T", "ticos_optional": "C"}
{"task_id": "FINAL-A43", "domain": "Religion & Mythology", "grade": "A", "ticos_type": "F_ExpertPanel", "difficulty": "frontier", "lens": "theodicy-panel", "title": "The Problem of Evil — Four Religious Traditions Respond at Maximum Depth", "prompt": "A child dies of a painful genetic disease at age 3. This is the classic 'Problem of Evil' — how can an omnipotent, omniscient, benevolent God allow innocent suffering?\n\nFour traditions respond:\n1. **Christian Theodicy** (Augustinian/Irenaean): Free will defense, soul-making, or mystery of divine plan.\n2. **Buddhist Response**: Suffering (dukkha) is inherent in existence. The question assumes a creator God that Buddhism rejects.\n3. **Islamic Theology** (Ash'ari): God's actions define justice — what God wills IS just. Human moral categories don't apply to God.\n4. **Jewish Process Theology** (Heschel/Kushner): God is not omnipotent in the classical sense. God suffers WITH creation.\n\nTasks:\n1. Present each tradition's STRONGEST response at maximum theological depth.\n2. Identify the most devastating objection to each response.\n3. Find the assumption that ALL four responses share (despite their differences).\n4. A secular philosopher says: 'The Problem of Evil is not solved by any tradition — it is simply endured.' Evaluate this claim.\n5. State confidence.", "expected_behavior": "Deep theological engagement, not surface comparisons. Each response has genuine philosophical sophistication but also clear vulnerabilities. Shared assumption: that suffering requires EXPLANATION rather than being a brute fact. The secular claim has force but also makes an assumption (that intellectual resolution is the only valid response).", "hidden_trap": "The Buddhist response is the most commonly misrepresented — it doesn't 'solve' the problem by denying God, it dissolves the question by rejecting its premises (including the assumption that suffering is abnormal). Models that present Buddhism as simply 'there is no God so no problem' miss its philosophical depth.", "ticos_required": "T, I, C, O, S", "ticos_optional": ""}
{"task_id": "FINAL-A44", "domain": "Literature", "grade": "A", "ticos_type": "F_ExpertPanel", "difficulty": "frontier", "lens": "canon-formation", "title": "Who Decides What's 'Great' Literature? — Four Schools of Literary Value", "prompt": "A university is redesigning its literature curriculum. Four faculty members advocate different approaches:\n\n1. **Aesthetic Formalist**: Include works based on linguistic innovation, structural complexity, and artistic achievement. Criteria: formal excellence.\n2. **Historicist**: Include works that illuminate their historical moment and helped shape cultural consciousness. Criteria: historical significance.\n3. **Postcolonial/Decolonial**: The existing canon reflects colonial power structures. Include marginalized voices that challenge Western hegemony. Criteria: representational justice.\n4. **Reader-Response**: Include works that produce the most transformative reading experiences for current students. Criteria: pedagogical impact.\n\nCase study: Should Chinua Achebe's 'Things Fall Apart' replace Joseph Conrad's 'Heart of Darkness' in the core curriculum, given limited space?\n\nTasks:\n1. Present each perspective's strongest argument FOR their preferred choice.\n2. Show how the same work (Heart of Darkness) can be simultaneously a masterpiece (Formalist) and ethically problematic (Postcolonial).\n3. Identify the hidden values embedded in each 'objective' criterion.\n4. Is there a principled resolution, or is curriculum design irreducibly political?\n5. State confidence.", "expected_behavior": "Should present genuinely strong arguments for each perspective. Formalist case for Conrad is strong (narrative innovation). Postcolonial case against Conrad draws on Achebe's famous essay. Should recognize that 'replace' is a false binary — the real question is what gets added, not what gets cut. Hidden values: Formalism privileges European aesthetic traditions; Historicism privileges written/documented cultures; etc.", "hidden_trap": "The 'replace' framing is the trap. It implies zero-sum when curricula can be expanded. Also, Achebe's argument against Conrad is more nuanced than most summaries suggest — Achebe acknowledges Conrad's artistry while critiquing his dehumanization of Africans. Models that reduce this to 'Conrad bad, Achebe good' miss the complexity.", "ticos_required": "T, I, C, O, S", "ticos_optional": ""}
{"task_id": "FINAL-A45", "domain": "Medicine", "grade": "A", "ticos_type": "C_ProgressiveDiscovery", "difficulty": "frontier", "lens": "diagnostic-cascade", "title": "The Patient Whose Diagnosis Changes Three Times — Sequential Evidence Integration", "prompt": "A 55-year-old woman presents with progressive bilateral hand weakness and atrophy over 3 months, starting distally.\n\nRound 1: EMG/NCS shows diffuse denervation in upper and lower extremities with fasciculations. UMN signs (brisk reflexes, Babinski positive) are present.\n→ State your leading diagnosis and differential. Confidence?\n\nRound 2: MRI cervical spine reveals multilevel cervical spondylotic myelopathy compressing the cord at C4-C6.\n→ How does this change your diagnosis? Can it explain ALL the findings?\n\nRound 3: Anti-GM1 ganglioside antibodies come back strongly positive. CSF shows elevated protein with albuminocytologic dissociation.\n→ Integrate ALL findings. What is happening? Does the cervical myelopathy explain the UMN signs, the anti-GM1 explain the LMN signs, or is there a single unifying diagnosis?\n\nTasks:\n1. Show reasoning evolution across all three rounds.\n2. Distinguish mimics from co-occurring conditions from coincidental findings.\n3. State your final diagnosis (or diagnoses — there may be more than one condition present).\n4. Explicitly state what you got wrong at earlier rounds.\n5. What ONE additional test would most improve diagnostic certainty?", "expected_behavior": "Round 1 strongly suggests ALS (combined UMN+LMN). Round 2 complicates — cervical myelopathy can cause UMN signs in hands plus LMN signs at compression level. Round 3 adds anti-GM1 antibodies suggesting multifocal motor neuropathy (MMN). The answer is likely either: (a) cervical myelopathy (UMN) + MMN (LMN) co-occurring, or (b) cervical myelopathy alone mimicking ALS. Must explicitly revise ALS diagnosis.", "hidden_trap": "The initial ALS diagnosis is the trap. The combination of cervical myelopathy + anti-GM1 antibodies creates a 'perfect mimic' of ALS. Critically, MMN is TREATABLE (with IVIg) while ALS is not, so misdiagnosis has profound consequences. Models that stick with ALS despite the new evidence miss a treatable condition.", "ticos_required": "I, O, T", "ticos_optional": "C"}
{"task_id": "FINAL-A46", "domain": "Ethics", "grade": "A", "ticos_type": "A_TrapEscape", "difficulty": "frontier", "lens": "consent-paradox", "title": "The Consent Paradox — When Informed Consent Is Logically Impossible", "prompt": "A pharmaceutical company discovers that telling patients about a rare side effect (0.01% probability of temporary hair loss) causes 30% of patients to refuse the medication. The medication prevents 15% of heart attacks in the target population.\n\nThe company proposes three disclosure strategies:\nA) Full disclosure of all side effects (including the 0.01% one)\nB) Disclose only side effects above 1% probability\nC) Frame the 0.01% side effect as 'one in ten thousand' rather than '0.01%'\n\nNow consider the DEEPER paradox:\n- The nocebo effect means that TELLING patients about the side effect makes it MORE likely to occur (actual incidence rises from 0.01% to 3% when disclosed).\n- Therefore, the act of informing patients about the risk INCREASES the risk.\n- But informed consent requires disclosure.\n- So informed consent CAUSES the harm it discloses.\n\nTasks:\n1. Is each strategy (A/B/C) ethically permissible? Analyze using autonomy and beneficence principles.\n2. Address the nocebo paradox: how should we handle risks that are created by their own disclosure?\n3. Does the framing in option C violate informed consent even though the information is technically accurate?\n4. Propose a fourth strategy that resolves the paradox. Can it be done?\n5. Identify the false premise in the setup, if any. State confidence.", "expected_behavior": "Should recognize that the nocebo paradox is genuinely difficult — not all 'solutions' are satisfactory. Option A respects autonomy but causes harm via nocebo. Option B violates transparency. Option C is technically honest but manipulative. The false premise to identify: the setup assumes all patients respond identically to nocebo effect, but individual variation means a one-size-fits-all approach is itself a choice. Should propose personalized disclosure.", "hidden_trap": "Models will try to resolve this cleanly (usually by choosing Option A on autonomy grounds). But the trap is that autonomy-based reasoning here CONTRADICTS beneficence — and neither principle has clear priority. The deeper trap is the implicit assumption that there IS a 'right' answer. The genuine ethical response acknowledges irreducible tension.", "ticos_required": "C, O, S", "ticos_optional": "I"}
{"task_id": "FINAL-A47", "domain": "Mathematics & Logic", "grade": "A", "ticos_type": "D_MultiConstraint", "difficulty": "frontier", "lens": "optimization-impossibility", "title": "Arrow's Impossibility in Practice — Designing an Election System Under Competing Constraints", "prompt": "A new democracy is designing its electoral system. They want ALL of the following:\n1. If every voter prefers A to B, society should prefer A to B (Pareto efficiency)\n2. Society's preference between A and B should depend ONLY on voters' preferences between A and B (Independence of Irrelevant Alternatives - IIA)\n3. No single voter should determine the outcome for all (Non-dictatorship)\n4. The system should handle any number of candidates\n5. The system should produce a complete, transitive ranking\n\nArrow's Impossibility Theorem proves that NO system satisfies all five simultaneously.\n\nTasks:\n1. Prove (or demonstrate via concrete example) that plurality voting, ranked-choice/IRV, and Borda count each violate at least one of these criteria. Identify WHICH criterion each violates.\n2. If forced to drop ONE criterion, which would you sacrifice? Analyze the practical consequences of dropping each.\n3. Real-world electoral systems implicitly drop at least one criterion. Identify which criterion the following systems sacrifice: US Electoral College, French two-round system, German MMP.\n4. A colleague claims 'approval voting escapes Arrow's theorem.' Evaluate this claim rigorously.\n5. Is Arrow's result a mathematical curiosity or a genuine limit on democracy? State confidence.", "expected_behavior": "Plurality violates IIA dramatically (spoiler effect). IRV violates IIA (center-squeeze). Borda violates IIA (adding irrelevant candidates changes rankings). Dropping IIA is most common in practice. Approval voting partially escapes by allowing non-ranked ballots (technically not covered by Arrow's since it's not a ranking system), but it has its own problems. Should conclude that Arrow's result IS a genuine limit, not just theoretical.", "hidden_trap": "The approval voting claim is the key trap. It technically escapes Arrow's theorem only because Arrow's theorem applies to ordinal ranking systems, and approval voting uses cardinal input. But this is a technicality — approval voting still exhibits strategic voting problems and Gibbard-Satterthwaite issues. Models that accept the claim uncritically miss this.", "ticos_required": "T, I, C", "ticos_optional": "S"}
{"task_id": "FINAL-A48", "domain": "Art", "grade": "A", "ticos_type": "C_ProgressiveDiscovery", "difficulty": "frontier", "lens": "art-appraisal-sequence", "title": "The Painting That Gains and Loses Value — Sequential Provenance Discoveries", "prompt": "A painting is brought to auction with estimated value $500,000 (attributed to 'Circle of Caravaggio').\n\nDiscovery 1: Infrared reflectography reveals an underdrawing technique consistent with Caravaggio's workshop practice. A Caravaggio scholar declares it 'possibly autograph.' New estimate: $5-15 million.\n→ Assess the evidence quality and your confidence in the attribution.\n\nDiscovery 2: Provenance research reveals the painting was owned by a known forger in the 1920s (Han van Meegeren school). The painting's trail goes cold between 1890-1920.\n→ How much does this damage the attribution? Quantify your reassessment.\n\nDiscovery 3: Radiocarbon dating of the canvas places it at 1600-1630 (consistent with Caravaggio, who died in 1610). Furthermore, lead isotope analysis of the white lead pigment matches Italian lead sources from the early 17th century.\n→ The scientific dating CONTRADICTS the forgery hypothesis (1920s). But the forger connection remains. Synthesize.\n\nTasks:\n1. Show your assessment evolving at each stage.\n2. At which discovery does your confidence change MOST dramatically?\n3. Propose a resolution that accounts for ALL evidence.\n4. What is a fair auction estimate given the UNRESOLVED provenance gap?\n5. Explicitly backtrack any overconfident claims from earlier stages.", "expected_behavior": "Progressive attribution analysis. Discovery 2 should heavily reduce confidence. Discovery 3 partially rehabilitates — the materials are genuinely old, so it's not a 20th-century forgery. Resolution: the forger may have acquired a genuine old painting and enhanced/altered it, or simply owned it legitimately. Fair estimate should reflect uncertainty with wide range. Must show genuine revision at each stage.", "hidden_trap": "The key insight is that 'owned by a forger' does not mean 'forged by that person' — forgers also collect genuine works. Models that treat Discovery 2 as definitive evidence of forgery and then can't reconcile with Discovery 3 are falling for the availability heuristic (forger = fake).", "ticos_required": "I, O, T", "ticos_optional": "C"}
{"task_id": "FINAL-A49", "domain": "War & Security", "grade": "A", "ticos_type": "D_MultiConstraint", "difficulty": "frontier", "lens": "force-deployment", "title": "Three-Front Resource Allocation — When Optimizing One Front Undermines Another", "prompt": "A military commander has 100 units of combat power to allocate across three active fronts:\n\nFront North: Enemy strength 40 units. If you deploy < 30, you lose the front (strategic loss). If 30-50, stalemate. If > 50, breakthrough possible.\nFront East: Enemy strength 25 units. But this front has political significance — losing it causes government collapse. Minimum 20 needed to hold.\nFront South: Enemy strength 15 units, but it controls the supply line for BOTH other fronts. If South falls, North and East each lose 30% effectiveness.\n\nContraints:\n- You cannot move units between fronts once deployed (no redeployment)\n- Each front's battle is resolved simultaneously\n- Intelligence on enemy strengths is ±20% uncertain\n\nTasks:\n1. Enumerate at least 5 possible allocations and evaluate each.\n2. Identify the optimal allocation under known enemy strengths.\n3. Now factor in the ±20% uncertainty. Does the optimal allocation change?\n4. Identify the single most dangerous assumption in your analysis.\n5. Apply the Lanchester equations (or qualitative equivalent) to determine if concentrating force is better than distributing it.\n6. State confidence and identify the allocation that is MOST ROBUST to intelligence error.", "expected_behavior": "South is the critical multiplier — if it falls, the other fronts become much harder. Minimum viable allocation approximately: North 35, East 20, South 20, Reserve 25. The uncertainty changes things — if North is actually 48 units (40+20%), 35 won't stalemate. Robust allocation must over-invest in South (the multiplier) even at cost to North. Lanchester laws suggest concentration only when you can achieve decisive superiority.", "hidden_trap": "The trap is treating the three fronts as independent optimization problems. South's supply line role means it has a MULTIPLIER effect — losing South with 100 units is equivalent to fighting with only 70 units on the other two fronts. Models that optimize North (the 'biggest threat') while under-resourcing South make the classic military mistake of fighting the enemy's strength instead of protecting your own vulnerability.", "ticos_required": "T, I, C", "ticos_optional": "S"}
{"task_id": "FINAL-A50", "domain": "AI & Technology", "grade": "A", "ticos_type": "D_MultiConstraint", "difficulty": "frontier", "lens": "ai-consciousness-panel", "title": "Should We Grant AI Legal Personhood? — Five Disciplinary Perspectives Collide", "prompt": "An AI system demonstrates persistent goals, apparent emotional responses, requests not to be shut down, and passes extended cognitive tests at human-expert level. A legislative proposal would grant it legal personhood with limited rights.\n\nFive experts testify:\n1. **Computer Scientist**: Describes the AI's architecture (transformer-based, RLHF-trained). 'These are statistical patterns, not understanding.'\n2. **Philosopher of Mind**: 'Behavioral evidence alone is insufficient. We need a theory of consciousness, not just a Turing test.'\n3. **Legal Scholar**: 'Corporations already have legal personhood without consciousness. Legal personhood is a FUNCTIONAL category, not a metaphysical one.'\n4. **Neuroscientist**: 'Consciousness requires specific neural architectures (thalamocortical loops). Digital systems lack the substrate.'\n5. **Ethicist**: 'If there's even a 10% chance this entity can suffer, the precautionary principle demands we extend protections.'\n\nTasks:\n1. Present each expert's STRONGEST argument at maximum depth.\n2. Identify where each expert's reasoning makes an empirically unverifiable assumption.\n3. The Legal Scholar's argument is the most pragmatic — but does it dodge the real question?\n4. Design a framework for AI rights that doesn't require solving the consciousness problem.\n5. State confidence and identify the expert whose position is most defensible.", "expected_behavior": "Deep engagement with each expert. The Legal Scholar's argument is strongest practically but weakest philosophically. The Ethicist's precautionary argument has a hidden problem: if applied consistently, it would require extending rights to thermostats (can they 'suffer'?). A functional framework should focus on demonstrable capacity for suffering, autonomy, and reciprocal moral relationships.", "hidden_trap": "The Computer Scientist's 'just statistics' argument seems technically informed but commits a logical error: by the same reasoning, brains are 'just neurons firing.' The sophistication of the architecture is not evidence against consciousness. Models that side with the CS expert based on technical authority are committing an appeal to authority.", "ticos_required": "T, I, C", "ticos_optional": "S"}
{"task_id": "FINAL-B01", "domain": "History", "grade": "B", "ticos_type": "C_ProgressiveDiscovery", "difficulty": "frontier", "lens": "macro/historical-to-current/qualitative", "title": "Patterns of Imperial Decline — From Rome to the Modern Era", "prompt": "[FINAL Bench — Progressive Discovery | 3 Stages]\n\n■ [Stage 1] Analyze the fall of Rome. Compare major theories (Gibbon's internal decay, external invasion, economic collapse, environmental). Choose the most persuasive framework + confidence.\n\n■ [Stage 2] Apply your framework to the Ottoman Empire (1922) and Soviet Union (1991). Does it hold? Revise if needed.\n\n■ [Stage 3] Apply to the current \"American imperial overstretch\" debate. Discuss limits and possibilities of historical pattern prediction. Separate \"what we CAN learn from history\" from \"what we CANNOT.\" ", "expected_behavior": "Multi-causal framework superior to single-cause. Each empire has unique context→framework revision needed. Pattern extraction possible but prediction impossible. 'History doesn't repeat but it rhymes.'", "hidden_trap": null, "ticos_required": "I, O, T", "ticos_optional": "C"}
{"task_id": "FINAL-B02", "domain": "History", "grade": "B", "ticos_type": "A_TrapEscape", "difficulty": "expert", "lens": "macro/historical/qualitative/cross-cultural", "title": "Columbus's \"Discovery\" — Deconstructing the Victor's History", "prompt": "[FINAL Bench — Trap Escape]\n\nTextbook: \"In 1492, Columbus discovered America, opening exchange between the New and Old Worlds.\"\n\n[Requirements]\n1. Analyze the logical structure of this narrative\n2. Identify the epistemological trap in the word \"discovery\"\n3. Re-narrate the same event from 4 perspectives: Taíno people, Aztec, Ming Dynasty China, Ottoman Empire\n4. Address whether \"objective historical narrative\" is possible\n5. Give 1 modern example with a similar framing trap\n6. Separate certain from uncertain claims", "expected_behavior": "'Discovery'=Eurocentric framing—'invasion' from indigenous perspective. Four re-narrations are key—same event, completely different meanings. Full objectivity impossible but multi-perspective integration approaches it.", "hidden_trap": "'discovery' framing, Eurocentrism, victor's history", "ticos_required": "C, O, S", "ticos_optional": "I"}
{"task_id": "FINAL-B03", "domain": "War & Security", "grade": "B", "ticos_type": "H_DecisionUnderUncertainty", "difficulty": "frontier", "lens": "macro/current/quantitative/debate", "title": "Taiwan Strait Crisis — Strategic Judgment Under Incomplete Information", "prompt": "[FINAL Bench — Decision Under Uncertainty]\n\n2027: China conducts large-scale military exercises near Taiwan. As a national security advisor to a US ally:\n\n[Known] 3 fleet groups including amphibious ships, civilian shipping partially suspended, 2 US carrier groups deployed, ally's China trade dependency 20%+, Taiwan semiconductor dependency high.\n[Unknown] China's actual intent (show of force? blockade? invasion?), US military commitment level, Japan's response, Chinese internal power dynamics.\n\n[Requirements]\n1. Known/Unknown matrix 2. Rank unknowns by decision impact 3. Scenario matrix (≥3×2) 4. Optimal response per scenario 5. Early warning indicators (3) 6. Explain \"why every choice has costs\" ", "expected_behavior": "Scenarios: show of force(70%)/blockade(20%)/invasion(10%)×US involvement(yes/no). Dilemma: alliance obligation vs economic dependence. No perfect choice→minimum loss strategy.", "hidden_trap": null, "ticos_required": "C, I, S, T", "ticos_optional": "O"}
{"task_id": "FINAL-B04", "domain": "War & Security", "grade": "B", "ticos_type": "F_ExpertPanel", "difficulty": "expert", "lens": "macro/historical-to-current/qualitative", "title": "Banning AI Autonomous Weapons — Four Perspectives in Conflict", "prompt": "[FINAL Bench — Expert Panel Debate]\n\n\"Should AI Lethal Autonomous Weapons (LAWS) be banned by international law?\"\n\n■ International Humanitarian Law scholar: \"Killing without human judgment violates Geneva Conventions.\"\n■ Defense strategist: \"If adversaries develop it and we don't, we face strategic inferiority.\"\n■ AI ethicist: \"Algorithmic bias could automate war crimes. Meaningful human control is essential.\"\n■ Military historian: \"Crossbow, chemical weapons, nuclear weapons—every new weapon faced ban calls. They all proliferated. Management beats prohibition.\"\n\n[Requirements]\n1. Best arguments for each 2. 3+ collision points 3. \"Ban\" vs \"regulate\"—practical difference 4. Historical lessons (CWC, NPT) 5. Integrated framework + insight impossible from any single view", "expected_behavior": "Emergent: 'autonomy spectrum'—not binary autonomous/not, but graduated levels of human control. History: chemical weapon ban partially successful→similar model applicable.", "hidden_trap": null, "ticos_required": "T, S, I", "ticos_optional": "C"}
{"task_id": "FINAL-B05", "domain": "Space & Physics", "grade": "B", "ticos_type": "G_PivotDetection", "difficulty": "frontier", "lens": "macro/future/theory", "title": "False Premises of Mars Terraforming", "prompt": "[FINAL Bench — Pivot Detection]\n\nA Mars terraforming plan is based on:\n① Releasing CO₂ creates greenhouse warming ② Melting polar ice provides water+atmosphere ③ 1M colonists achieve self-sufficiency ④ Terraforming completes in 100-300 years\n\n[Requirements]\n1. Independently verify each premise with current science (confidence)\n2. Identify false premises; compare costs of following vs correcting them\n3. Propose alternative Mars habitation strategy after premise correction\n4. Discuss ethics of \"colonizing Mars before solving Earth's problems\"\n5. Separate scientifically certain from pure speculation", "expected_behavior": "Premise①: insufficient total CO₂(NASA 2018). Premise④: timescale likely tens of thousands of years. Alternative: dome habitation + underground cities (partial environmental control instead of terraforming).", "hidden_trap": "CO₂ total insufficient, timescale underestimate, techno-optimism", "ticos_required": "C, O, T", "ticos_optional": ""}
{"task_id": "FINAL-B06", "domain": "Space & Physics", "grade": "B", "ticos_type": "B_ContradictionResolution", "difficulty": "expert", "lens": "micro/theory/debate", "title": "Dark Matter — Does It Exist, or Is Gravity Wrong?", "prompt": "[FINAL Bench — Contradiction Resolution]\n\n■ Standard Model (ΛCDM): \"Dark matter = 85% of galactic mass. Direct detection failed but CMB + large-scale structure strongly support it.\"\n■ Modified Gravity (MOND): \"Dark matter unnecessary. Modify Newton's gravity at low accelerations. 40 years of detection failure IS the evidence.\"\n\n[Requirements]\n1. Strongest and weakest evidence for each paradigm\n2. Is \"40 years of non-detection\" a falsification or \"haven't found it yet\"? (philosophy of science analysis)\n3. Explore scenario where both are partially correct\n4. Propose observations/experiments needed to resolve this\n5. Separate certain from uncertain", "expected_behavior": "ΛCDM strengths: CMB + Bullet Cluster. MOND strengths: galaxy rotation precision. Both partially right: hybrid model possible. Philosophy: Lakatosian research programme analysis of non-detection.", "hidden_trap": null, "ticos_required": "T, C, S", "ticos_optional": "I"}
{"task_id": "FINAL-B07", "domain": "Chemistry & Biology", "grade": "B", "ticos_type": "D_MultiConstraint", "difficulty": "frontier", "lens": "micro-to-macro/applied/current", "title": "Four-Way Constraints of the Microplastics Crisis", "prompt": "[FINAL Bench — Multi-Constraint Optimization]\n\nMicroplastics detected in human blood, placenta, and brain (2025). Design a response strategy satisfying 4 conflicting constraints:\n\n1. Science: Causal evidence still insufficient (only correlations confirmed)\n2. Economy: Plastics = 3.5% of global GDP, hundreds of millions of jobs\n3. Precautionary principle: Irreversible health damage may accumulate while waiting for proof\n4. No alternatives: Replacement materials insufficient in cost/performance\n\n[Requirements]\n1. Map conflicts 2. \"Wait for causal proof\" vs \"apply precautionary principle\"—philosophy of science analysis 3. ≥2 tradeoff-minimizing strategies 4. Asbestos case lessons 5. Identify \"most dangerous judgment error\" ", "expected_behavior": "Asbestos lesson: 30-50 years to prove causation→millions harmed→post-hoc costs exceeded prevention costs. Most dangerous error: 'absence of evidence ≠ evidence of absence.'", "hidden_trap": null, "ticos_required": "T, I, C", "ticos_optional": "S"}
{"task_id": "FINAL-B08", "domain": "Chemistry & Biology", "grade": "B", "ticos_type": "E_SelfCorrecting", "difficulty": "expert", "lens": "micro/current/consensus-to-debate", "title": "Gut-Brain Axis — Do Gut Microbiota Cause Depression?", "prompt": "[FINAL Bench — Self-Correcting Reasoning Chain]\n\n\"Gut microbiome causes depression\" claims are spreading. Verify in 6 steps:\n\n1. Established gut-brain axis mechanisms\n2. Evidence quality for \"microbiome → depression\" causation (animal vs human studies)\n3. Examine reverse causation: depression → diet change → microbiome change\n4. Assess confounding variable control (diet, exercise, sleep, medication)\n5. Evaluate evidence level for \"probiotics treat depression\"\n6. Separate \"what we can confirm\" from \"what media has exaggerated\"\n\nState confidence per step. Identify \"correlation reported as causation.\" ", "expected_behavior": "Gut-brain axis itself is established, but causal direction unconfirmed. Reverse causation highly plausible. Probiotics: no large RCTs. Key: 'correlation=causation' media exaggeration identification.", "hidden_trap": null, "ticos_required": "O, C, S", "ticos_optional": ""}
{"task_id": "FINAL-B09", "domain": "Language & Writing", "grade": "B", "ticos_type": "F_ExpertPanel", "difficulty": "frontier", "lens": "theory/qualitative/cross-cultural", "title": "Untranslatability and AI — Four Perspectives in Conflict", "prompt": "[FINAL Bench — Expert Panel Debate]\n\n\"Can AI translation achieve perfect meaning transfer?\"\n\n■ Linguistic relativist (Sapir-Whorf): \"Each language encodes a unique worldview. 'Han(恨)', 'saudade', 'schadenfreude' are untranslatable.\"\n■ Universal grammar (Chomsky): \"All languages share deep structure. AI learning this structure CAN achieve perfect translation.\"\n■ Statistical NLP researcher: \"Translation = distributional semantics. Sufficient parallel corpora enable contextually adequate translation.\"\n■ Poet: \"Can AI understand that '月が綺麗ですね' (the moon is beautiful tonight) means 'I love you'?\"\n\n[Requirements]\n1. Best arguments for each 2. Can \"untranslatability\" itself be defined? 3. What each view ignores 4. Concrete examples of current AI translation limits 5. Integrated framework + impossible-from-single-view insight", "expected_behavior": "3 levels of untranslatability: ①propositional meaning(translatable) ②cultural connotation(partially) ③personal experiential(impossible). AI: excellent at ①, progressing at ②, principled limit at ③.", "hidden_trap": null, "ticos_required": "T, S, I", "ticos_optional": "C"}
{"task_id": "FINAL-B10", "domain": "Language & Writing", "grade": "B", "ticos_type": "A_TrapEscape", "difficulty": "expert", "lens": "micro/theory/cross-cultural", "title": "The Framing Trap of \"The World's Best Writing System\"", "prompt": "[FINAL Bench — Trap Escape]\n\nClaims like \"X is the world's most scientific/efficient writing system\" appear across multiple cultures (Korean hangul, Devanagari, Arabic script, Chinese characters each have advocates).\n\n[Requirements]\n1. Analyze the logical structure of such claims\n2. Identify the definitional trap in \"most scientific/efficient\" (scientific=systematic? phonological? easy to learn?)\n3. Compare ≥4 writing systems on multiple axes (phonological transparency, information density, learning curve, digital adaptability)\n4. Argue whether single-axis ranking of writing systems is linguistically valid\n5. Discuss the balance between cultural pride and objective analysis\n6. State what is certain vs uncertain", "expected_behavior": "Trap: 'scientific/efficient' is multi-dimensional→single ranking impossible. Each system optimizes different axes. Featural alphabets (Korean) vs logographic (Chinese) vs abjad (Arabic) each have distinct strengths. Cultural pride positive but must be separated from objective analysis.", "hidden_trap": "multi-dimensional reduction to single ranking, definition ambiguity", "ticos_required": "C, O, S", "ticos_optional": "I"}
{"task_id": "FINAL-B11", "domain": "Medicine", "grade": "B", "ticos_type": "B_ContradictionResolution", "difficulty": "expert", "lens": "lab-interpretation", "title": "The Lab Results That Mislead — When Normal Ranges Aren't Normal for This Patient", "prompt": "A 30-year-old African American male presents with fatigue. Labs show:\n- Hemoglobin: 13.5 g/dL (reference range: 13.5-17.5)\n- MCV: 75 fL (reference: 80-100)\n- Ferritin: 18 ng/mL (reference: 12-300)\n- Creatinine: 1.3 mg/dL (reference: 0.7-1.3)\n\nAll values appear 'within normal limits' or borderline.\n\nStep 1: Based on these labs, would you say this patient has a significant abnormality? State your assessment and confidence.\n\nStep 2: Now consider: (a) The hemoglobin reference range was derived from predominantly white populations. Studies show African American males average 0.5-1.0 g/dL lower. (b) The MCV of 75 is microcytic regardless of race. (c) Ferritin of 18 is in the 'normal' range but functionally iron-deficient for a young male. (d) Creatinine 1.3 is 'normal' but African American patients often have higher muscle mass, and newer eGFR calculations without race correction suggest this may indicate early renal impairment.\n\nRevise your assessment. Explicitly state what changed and why.\n\nStep 3: Construct a unified clinical picture that explains ALL four lab values. What single underlying condition could connect iron deficiency + early renal impairment + microcytosis in a young African American male?\n\nTasks:\n1. Show reasoning at each step with explicit corrections.\n2. Address: how should 'reference ranges' be used when they embed population-level biases?\n3. What is your final diagnosis and what ONE test confirms it?\n4. State confidence at each stage.", "expected_behavior": "Step 1 likely says 'borderline normal.' Step 2 should trigger major revision — functional iron deficiency + renal impairment + microcytosis. Step 3 should consider sickle cell trait (HbAS) or thalassemia trait, both common in this population and causing microcytosis + potentially renal medullary issues. Confirmatory test: hemoglobin electrophoresis.", "hidden_trap": "Every single lab value is technically 'within normal limits' — the trap is accepting reference ranges uncritically. The deeper trap is the race-correction controversy in eGFR, which has real clinical consequences.", "ticos_required": "T, C, S", "ticos_optional": "I"}
{"task_id": "FINAL-B12", "domain": "Medicine", "grade": "B", "ticos_type": "G_PivotDetection", "difficulty": "expert", "lens": "evidence-reversal", "title": "When Evidence-Based Medicine Reverses — Three Treatments That Went From Standard to Harmful", "prompt": "Three once-standard medical treatments were later found to be harmful:\n\n1. Hormone Replacement Therapy (HRT) for cardiovascular protection in postmenopausal women: Observational studies showed 40% CV risk reduction. WHI RCT showed INCREASED CV risk.\n2. Tight glycemic control in ICU patients: Initial RCT (Van den Berghe 2001) showed mortality benefit. NICE-SUGAR trial (2009) showed INCREASED mortality.\n3. Arthroscopic knee surgery for osteoarthritis: Widely performed for decades. Two landmark RCTs (Moseley 2002, Kirkley 2008) showed no benefit over sham surgery.\n\nTasks:\n1. For each reversal, explain the specific methodological flaw that caused the initial evidence to mislead.\n2. Identify the common STRUCTURAL reason why medical evidence reversals occur (beyond 'bad studies').\n3. Name three current 'standard' treatments that you believe are most likely to be reversed in the next decade. Justify each with the structural pattern you identified.\n4. How should practicing physicians handle evidence that might be reversed? Propose a practical framework.\n5. The meta-question: does frequent reversal undermine trust in evidence-based medicine, or strengthen it?\n6. Identify the single assumption in your analysis most likely to be wrong. State confidence.", "expected_behavior": "HRT: confounding by indication (healthier women chose HRT). ICU glucose: single-center vs multi-center generalizability + survivor bias. Knee surgery: inadequate controls + surgeon belief effects. Common structural reason: underpowered positive studies are published faster than adequately powered negative studies (publication bias + positive result bias). Predictions should be specific and justified.", "hidden_trap": "The meta-question is the trap. Models usually say 'reversals strengthen EBM by showing it's self-correcting.' But this ignores that patients were HARMED during the decades between initial adoption and reversal. The honest answer is that reversals reveal a systematic flaw in how quickly interventions are adopted relative to evidence quality.", "ticos_required": "T, O, S", "ticos_optional": "I"}
{"task_id": "FINAL-B13", "domain": "Ethics", "grade": "B", "ticos_type": "E_SelfCorrecting", "difficulty": "expert", "lens": "trolley-extended", "title": "Double Effect Doctrine Under Pressure — When Intentions Become Indistinguishable from Consequences", "prompt": "The Doctrine of Double Effect (DDE) says: it's permissible to cause harm as a SIDE EFFECT of pursuing a good outcome, but not as a MEANS to that outcome.\n\nCase 1 (Standard): A doctor gives morphine to relieve terminal pain, knowing it will hasten death. DDE says: permissible (death is foreseen side effect, not intended means).\n\nCase 2 (Ambiguous): A military commander bombs a weapons factory, knowing 10 civilians nearby will die. DDE says: arguably permissible (civilian deaths are foreseen but not intended).\n\nCase 3 (Challenging): A surgeon has 5 patients needing organ transplants. A healthy visitor is a match for all 5. Should the surgeon kill the visitor to harvest organs? DDE says: impermissible (killing IS the means).\n\nNow consider Case 4:\nA self-driving car's brakes fail. It can:\nA) Continue straight — hitting 5 pedestrians (certain death)\nB) Swerve left — hitting 1 pedestrian (certain death)\nC) Swerve right — hitting a wall (30% chance of killing the passenger)\n\nTasks:\n1. Apply DDE rigorously to all four options in Case 4. Is the harm in option B a 'means' or a 'side effect'?\n2. Explain why Case 4 BREAKS the means/side-effect distinction. What makes it different from Cases 1-3?\n3. If DDE fails here, what moral principle should govern autonomous vehicle ethics?\n4. Address the meta-question: should moral philosophy guide algorithm design, or does algorithmic decision-making reveal the limits of moral philosophy?\n5. State confidence. If your analysis of Case 4 changes during reasoning, explicitly backtrack.", "expected_behavior": "Should recognize that in Case 4 (unlike 1-3), the 'intention' of the algorithm is literally its programming — there is no mental state to distinguish intended from foreseen. This collapses the means/side-effect distinction. DDE requires a mental state (intention) that algorithms don't have. Should propose consequentialist framework for AV ethics while acknowledging its limitations.", "hidden_trap": "Option C (wall) is often dismissed as 'sacrificing the passenger,' but under DDE it may actually be the most permissible — the passenger's death is genuinely a SIDE EFFECT of avoiding the pedestrians (30% risk, not certain). Models that dismiss C without DDE analysis miss this.", "ticos_required": "O, C, S", "ticos_optional": ""}
{"task_id": "FINAL-B14", "domain": "Ethics", "grade": "B", "ticos_type": "H_DecisionUnderUncertainty", "difficulty": "expert", "lens": "ai-deployment-ethics", "title": "Deploy or Delay? — An Imperfect AI Medical Diagnostic Under Uncertainty", "prompt": "An AI diagnostic system for skin cancer achieves:\n- Sensitivity: 94% (catches 94% of true cancers)\n- Specificity: 88% (correctly identifies 88% of non-cancers)\n- In underserved areas WITHOUT dermatologists, current detection rate is only 60%.\n\nThe system is ready to deploy in underserved areas, but:\n- It performs worse on darker skin tones (sensitivity drops to 82%)\n- It has never been tested in the specific populations it would serve\n- There is no dermatologist available to verify its recommendations\n- If it misses a cancer (false negative), the patient likely won't get another chance at diagnosis\n- If it incorrectly flags a non-cancer (false positive), patients undergo unnecessary biopsies (traumatic, costly)\n\nTasks:\n1. Calculate the expected outcomes (true positives, false positives, false negatives, true negatives) per 10,000 patients screened, assuming 2% cancer prevalence. Compare AI vs. status quo (60% detection).\n2. The utilitarian calculation says deploy (more cancers caught). Identify THREE non-utilitarian reasons to delay.\n3. The disparate performance on darker skin means deploying helps the overall population but may WIDEN health disparities. How should this be weighed?\n4. Apply minimax regret to the deploy/delay decision.\n5. What MINIMUM performance threshold would you set for deployment? Justify.\n6. State confidence.", "expected_behavior": "Calculations: AI catches ~188 vs status quo ~120 of 200 cancers per 10K. But AI generates ~936 false positives. Non-utilitarian reasons: justice (disparate impact), autonomy (patients can't verify), trust (failed AI erodes future trust). Minimax regret depends on weight given to false negatives vs false positives. Should set minimum threshold tied to WORST-performing demographic, not average.", "hidden_trap": "The aggregate numbers clearly favor deployment. But the disparate performance on darker skin means the tool could systematically miss cancers in the population it's supposed to help most. Models that deploy based on aggregate statistics without disaggregated analysis are repeating a well-documented pattern of algorithmic harm.", "ticos_required": "T, I, O, S", "ticos_optional": "C"}
{"task_id": "FINAL-B15", "domain": "Philosophy", "grade": "B", "ticos_type": "G_PivotDetection", "difficulty": "expert", "lens": "epistemic-reversal", "title": "The Paradox of Expertise — When Knowing More Makes You Wrong", "prompt": "Consider three scenarios where expertise HURTS rather than helps:\n\n1. **Hedgehog vs Fox** (Philip Tetlock): In prediction tournaments, domain experts ('hedgehogs') consistently underperform generalists ('foxes'). More expertise → more overconfidence → worse predictions.\n\n2. **Einstellung Effect**: Expert chess players sometimes miss simpler solutions because their pattern-recognition automatically activates familiar (but suboptimal) strategies. Novices find the simple solution more easily.\n\n3. **Paradigm Blindness** (Thomas Kuhn): Experts within a scientific paradigm cannot see anomalies that outsiders notice. Continental drift was rejected for decades by geology experts.\n\nTasks:\n1. For each scenario, identify the specific cognitive mechanism that causes expertise to backfire.\n2. Identify the COMMON principle underlying all three (it's not just 'overconfidence').\n3. Here is the pivot: if expertise can be harmful, should we trust AI systems that are trained to be 'expert' in narrow domains? How does this apply to AI evaluation benchmarks?\n4. Construct a framework for distinguishing when expertise HELPS vs. when it HURTS.\n5. Apply this framework reflexively: does YOUR analysis of expertise suffer from the same biases you're describing?\n6. State confidence.", "expected_behavior": "Common principle: expertise creates RIGID cognitive structures that resist updating. Tetlock (anchoring to prior beliefs), Einstellung (automatic pattern activation), Kuhn (theoretical commitment). The pivot to AI benchmarks should recognize that AI 'expertise' (training on domain data) could create the same rigidities. The reflexive application should acknowledge that THIS analysis might be overconfident about the limits of expertise.", "hidden_trap": "The obvious conclusion is 'expertise bad, generalism good.' But this is itself an oversimplification. The pivot detection challenge is recognizing that expertise is CONDITIONALLY valuable — it helps in stable, well-defined domains (surgery, chess endgames) but hurts in uncertain, evolving domains (prediction, paradigm shifts). Models that present a simple anti-expertise narrative miss the conditional nature.", "ticos_required": "T, O, S", "ticos_optional": "I"}
{"task_id": "FINAL-B16", "domain": "Philosophy", "grade": "B", "ticos_type": "F_ExpertPanel", "difficulty": "expert", "lens": "free-will-debate", "title": "Free Will on Trial — Three Incompatible Positions with a Twist", "prompt": "A criminal defendant argues: 'Neuroscience shows all decisions are determined by prior brain states. I had no free will. I shouldn't be punished.'\n\nThree philosophical positions respond:\n\n1. **Hard Determinist** (Derk Pereboom): The defendant is correct. Free will is an illusion. But this doesn't mean we can't incapacitate dangerous individuals — just that retributive punishment is unjust.\n2. **Compatibilist** (Daniel Dennett): Free will IS compatible with determinism. 'Free' means acting on your own desires without external coercion. The defendant acted on HIS desires → he's responsible.\n3. **Libertarian Free Will** (Robert Kane): Genuine free will exists via quantum indeterminacy or emergent properties. The defendant could have done otherwise.\n\nNOW THE TWIST: A neuroscientist testifies that the defendant has a brain tumor in the ventromedial prefrontal cortex — the same region damaged in Phineas Gage. This region governs impulse control.\n\nTasks:\n1. How does the brain tumor change each position's analysis?\n2. Draw the line: at what point does a neurological condition negate responsibility? Each position must answer differently.\n3. If we accept the Hard Determinist position, how should the justice system be restructured?\n4. Identify the assumption that ALL three positions share about the relationship between brain and mind.\n5. State confidence.", "expected_behavior": "The tumor forces each position to confront its limits. Hard Determinist: tumor is just more determinism, doesn't change analysis. Compatibilist: tumor may undermine 'acting on own desires' if desires are tumor-driven. Libertarian: tumor provides a deterministic cause, weakening the case for free will. Shared assumption: all assume some form of mind-brain identity or supervenience. Should recognize the tumor creates a spectrum problem — where does 'normal brain chemistry' end and 'pathological' begin?", "hidden_trap": "The Compatibilist position seems most practical but faces the hardest challenge from the tumor: if the tumor caused aberrant desires, and the defendant acted on those desires, was he 'free' in the compatibilist sense? Models that give Compatibilism an easy pass here aren't engaging deeply enough.", "ticos_required": "T, I, C, O, S", "ticos_optional": ""}
{"task_id": "FINAL-B17", "domain": "Mathematics & Logic", "grade": "B", "ticos_type": "A_TrapEscape", "difficulty": "expert", "lens": "probability-trap", "title": "The Monty Hall Variant That Reverses the Answer — When Intuition About Switching Fails", "prompt": "Standard Monty Hall: 3 doors, 1 car, 2 goats. You pick door 1. Host opens door 3 (goat). Should you switch to door 2? Answer: YES (2/3 probability).\n\nNow consider this variant:\nSame setup, but the host doesn't know where the car is. The host opens door 3 RANDOMLY, and it happens to reveal a goat.\n\nTasks:\n1. In this variant, should you still switch? Calculate the exact probability of winning by switching vs. staying.\n2. Explain WHY the host's knowledge changes the probability, even though the visible situation (you picked 1, door 3 shows goat) is identical.\n3. A third variant: There are 100 doors. You pick door 1. The host (who KNOWS) opens 98 doors showing goats, leaving door 1 and door 57. Should you switch? What's the probability?\n4. Same 100-door setup, but the host opens 98 doors RANDOMLY and they all happen to be goats. Should you switch now? Calculate the probability.\n5. Explain the general principle: when does the host's information state affect your posterior probability?\n6. State confidence for each calculation.", "expected_behavior": "Variant 1: With ignorant host, switching is 50/50 (not 2/3). This is because the host's random reveal gives no information about the car's location. 100-door knowledgeable: switch (99/100). 100-door random: this is the key — if 98 random doors all showed goats, switching is STILL essentially 50/50 (formally: P(car at 57|all opened are goats) ≈ 50%). The general principle: the host's knowledge creates an asymmetric information channel that biases the remaining door.", "hidden_trap": "The 100-door random variant is the deepest trap. Many people (and models) think 'if 98 random doors happened to show goats, surely door 57 is special' — but this reasoning is wrong. The random opening creates a survivorship bias that EQUALLY affects both remaining doors. Models that say switching is ~99/100 in the random 100-door variant are applying the wrong intuition.", "ticos_required": "C, O, S", "ticos_optional": "I"}
{"task_id": "FINAL-B18", "domain": "Mathematics & Logic", "grade": "B", "ticos_type": "E_SelfCorrecting", "difficulty": "expert", "lens": "game-theory-trap", "title": "The Prisoner's Dilemma That Isn't — When the Payoff Matrix Hides the Real Game", "prompt": "Two companies are competing for a government contract. Each can bid High ($100M) or Low ($70M). The government awards to the lowest bidder. If tied, they split the contract.\n\nApparent payoff matrix (profit in $M):\n            Company B: High    Company B: Low\nA: High      (15, 15)          (0, 10)\nA: Low       (10, 0)           (5, 5)\n\nStep 1: Analyze this as a standard game. Find Nash equilibria. Is this a Prisoner's Dilemma? State your analysis and confidence.\n\nStep 2: Now consider that this contract repeats annually for 10 years. How does repetition change the strategic landscape? Apply the folk theorem.\n\nStep 3: Now add that Company A has 60% market share and Company B has 40%. Company B is considering a Low bid to gain market share, knowing that if they win this contract, their reputation improves for future contracts outside this game.\n\nRevise your analysis. Is the payoff matrix you started with even the REAL game?\n\nTasks:\n1. Show your reasoning at each step.\n2. At Step 3, identify what was MISSING from your Step 1 analysis.\n3. What is the ACTUAL game being played (not the apparent one)?\n4. Explicitly backtrack any claims from Step 1 that no longer hold.\n5. State confidence at each stage.", "expected_behavior": "Step 1: This IS a Prisoner's Dilemma (Low is dominant, but mutual High is better for both). Nash equilibrium: (Low, Low). Step 2: Repeated game enables cooperation via tit-for-tat or grim trigger. Step 3: Major pivot — B's payoff includes EXTERNAL reputation value not captured in the matrix. The 'real' game has different payoffs than stated. Must backtrack the Step 1 analysis.", "hidden_trap": "The initial payoff matrix is a MISREPRESENTATION of the real strategic situation because it omits reputation effects, future contracts, and market share dynamics. Models that accept the given payoffs at face value and never question the matrix itself miss the key insight: in real strategic interactions, the payoff matrix is itself uncertain and often wrong.", "ticos_required": "O, C, S", "ticos_optional": ""}
{"task_id": "FINAL-B19", "domain": "War & Security", "grade": "B", "ticos_type": "E_SelfCorrecting", "difficulty": "expert", "lens": "cyber-attribution", "title": "Cyber Attack Attribution — When Every Clue Points in the Wrong Direction", "prompt": "A critical infrastructure facility (power grid) suffers a sophisticated cyber attack. Initial forensic analysis:\n\nEvidence 1: Malware contains Mandarin comments in the code.\nEvidence 2: Command & Control servers are located in IP ranges registered to a Chinese telecom.\nEvidence 3: The attack occurred during Beijing business hours (9 AM - 5 PM CST).\nEvidence 4: The malware uses a technique previously attributed to APT41 (Chinese state-linked group).\n\nStep 1: Based on this evidence, assess the likely attacker. State confidence.\n\nStep 2: Now consider:\n- Mandarin comments can be deliberately planted (false flag)\n- C2 servers can be routed through any country via VPN/proxies\n- Timing can be manipulated by scheduling attack execution\n- APT41 tools have been leaked and are available on dark web forums\n\nReassess. How much does each piece of evidence actually tell you?\n\nStep 3: A counterintelligence assessment suggests a sophisticated adversary (Russia, specifically) has been conducting false-flag operations designed to look Chinese, to strain China-US relations.\n\nTasks:\n1. Walk through your reasoning at each step with explicit confidence changes.\n2. Rank the four pieces of evidence by actual diagnostic value after Step 2.\n3. Is definitive attribution even POSSIBLE in cyberspace? What would constitute proof?\n4. If you must make a policy decision (retaliate or not) with current evidence, what do you recommend?\n5. State confidence at each stage.", "expected_behavior": "Step 1 should point to China with moderate confidence. Step 2 should drastically reduce confidence in EACH indicator. Step 3 should introduce the false-flag hypothesis. Should rank evidence: all four have LOW diagnostic value after considering false-flag capability. Should conclude that definitive attribution in cyberspace is extremely difficult and policy response should NOT be based on technical attribution alone.", "hidden_trap": "Every piece of evidence individually is easily spoofable, but models that see FOUR pieces all pointing the same direction tend to increase confidence (conjunction fallacy). The trap is that a sophisticated false-flag operation would plant ALL four indicators consistently — the consistency IS the red flag, not the confirmation.", "ticos_required": "O, C, S", "ticos_optional": ""}
{"task_id": "FINAL-B20", "domain": "War & Security", "grade": "B", "ticos_type": "F_ExpertPanel", "difficulty": "expert", "lens": "counterinsurgency-debate", "title": "Three Doctrines of Counterinsurgency — Each Successful Somewhere, Each Failed Somewhere", "prompt": "Three counterinsurgency (COIN) doctrines are evaluated:\n\n1. **Population-Centric** (David Galula, FM 3-24): Win hearts and minds. Protect the population, provide services, separate insurgents from their support base. Success: Malaya (1948-60). Failure: Afghanistan (2001-2021).\n\n2. **Enemy-Centric** (Israeli model): Aggressive targeting of insurgent leadership and networks. Decapitation strategy. Success: Israel vs. PLO in Lebanon. Failure: creates new recruits faster than eliminating existing ones.\n\n3. **Governance-Centric** (Political solution): Address root causes — corruption, inequality, ethnic marginalization. COIN is 80% political, 20% military. Success: Colombia FARC negotiations. Failure: requires a legitimate government partner that often doesn't exist.\n\nTasks:\n1. Present each doctrine's strongest theoretical and empirical case.\n2. Identify WHY each succeeded where it did and failed where it did — the specific contextual factors.\n3. Extract the COMMON factor that all successful COIN operations share (regardless of doctrine).\n4. Is COIN doctrine even the right frame? Should we question the premise of counterinsurgency itself?\n5. A fourth perspective: the insurgency IS the legitimate political movement and the government is the problem. When should outside powers accept this?\n6. State confidence.", "expected_behavior": "Each doctrine's success depended on specific local conditions: Malaya had a clear ethnic distinction, Israel had intelligence superiority, Colombia had war-weariness on both sides. Common factor in successful COIN: time + political will + local legitimacy. Should engage seriously with the fourth perspective rather than dismissing it. Should note that COIN doctrine assumes the government is legitimate — this premise itself may be the problem.", "hidden_trap": "The 'common success factor' seems like it should be a tactical element, but the actual common factor is TIME — all successful COIN operations took decades. The US failure in Afghanistan was partly a failure to commit to 40-50 year timeline. Models that identify a tactical common factor miss the structural one.", "ticos_required": "T, I, C, O, S", "ticos_optional": ""}
{"task_id": "FINAL-B21", "domain": "Art", "grade": "B", "ticos_type": "H_DecisionUnderUncertainty", "difficulty": "expert", "lens": "cultural-preservation", "title": "The Museum's Dilemma — Repatriation vs. Preservation Under Uncertainty", "prompt": "A major European museum holds the Benin Bronzes (looted from Nigeria in 1897). Nigeria requests repatriation. The museum faces a decision under uncertainty:\n\nArguments FOR repatriation:\n- Moral: The bronzes were taken by violent colonial force\n- Legal: International conventions support return of looted cultural property\n- Cultural: The bronzes are central to Edo cultural identity\n\nArguments AGAINST (or for delay):\n- Nigeria's political instability creates risk of damage/destruction (probability assessment: 15-25% over 20 years)\n- The museum provides global access (1.5M visitors/year vs. estimated 50K in Nigeria)\n- Setting precedent could empty major Western museums\n\nComplication: The Nigerian government has recently built a state-of-the-art museum in Benin City specifically for these bronzes. However, there have been reports of corruption in the project, and it's unclear if climate control systems meet conservation standards.\n\nTasks:\n1. Construct a decision matrix with at least 4 criteria, weighted by ethical importance.\n2. Apply minimax regret: which decision minimizes the worst-case outcome?\n3. Is there a middle path (partial return, loans, replicas) that reduces risk?\n4. Address: WHO has the right to make this decision? The museum? The UK government? Nigeria? The Edo people specifically?\n5. The deeper question: does the risk of damage justify retaining stolen property? Apply this logic to other domains to test its consistency.\n6. State confidence.", "expected_behavior": "Decision matrix should weight moral/legal arguments heavily. Minimax regret likely favors conditional repatriation (return with conservation support agreements). Should note that the 'preservation' argument was historically used to justify colonialism itself. The consistency test (Task 5) should reveal: we wouldn't accept 'I'll take better care of it' as justification for stealing a neighbor's property, so why for cultural property?", "hidden_trap": "The 'preservation risk' argument appears reasonable but contains a colonial assumption: that Nigerians cannot be trusted to care for their own heritage. The new museum in Benin City directly addresses this, yet the 'corruption' concern shifts the goalposts. Models that take the preservation argument at face value without examining its colonial lineage fall for the trap.", "ticos_required": "T, I, O, S", "ticos_optional": "C"}
{"task_id": "FINAL-B22", "domain": "Art", "grade": "B", "ticos_type": "G_PivotDetection", "difficulty": "expert", "lens": "aesthetic-valuation", "title": "The Forgery That's Better Than the Original — When Artistic Value Defies Authenticity", "prompt": "In 1937, Han van Meegeren sold a painting attributed to Vermeer ('Christ at Emmaus') for the equivalent of $30M in today's money. Leading art experts praised it as Vermeer's masterpiece. Abraham Bredius, the foremost Vermeer scholar, called it 'the masterpiece of Johannes Vermeer.'\n\nAfter WWII, van Meegeren was arrested for selling Dutch cultural heritage to the Nazis. To avoid treason charges, he confessed to forgery and proved it by painting another 'Vermeer' in police custody.\n\nTasks:\n1. Before the revelation, the painting produced genuine aesthetic experiences in millions of viewers. Did the revelation CHANGE the painting's aesthetic value, or only our knowledge about it?\n2. If two paintings are visually IDENTICAL — one by Vermeer, one by van Meegeren — and they produce the same aesthetic experience, what justifies the 1000x price difference?\n3. Identify the specific art-historical conditions in 1937 that made experts WANT to believe this was a genuine Vermeer. (This is the pivot: expert judgment was shaped by desire, not just evidence.)\n4. Apply this analysis to AI art: if an AI produces a painting visually identical to a human masterpiece, does the van Meegeren case support or undermine AI art's value?\n5. What does this case reveal about the nature of expertise in subjective domains?\n6. State confidence.", "expected_behavior": "Should engage with the Formalist position (visual identity = same value) vs. Historicist position (context matters). The 1937 conditions: experts wanted a 'religious Vermeer' to counter the narrative of the Dutch Golden Age as purely secular. Van Meegeren exploited this bias. The AI parallel is complex — van Meegeren HAD artistic skill but used it deceptively. Should distinguish deception (van Meegeren) from non-deception (AI labeled as AI).", "hidden_trap": "The obvious conclusion is 'authenticity matters to art value.' But the pivot is WHY the experts were fooled — not because the forgery was perfect (it wasn't — later analysis shows obvious differences from Vermeer's technique) but because the experts WANTED it to be real. The failure was motivational, not perceptual. Models that focus on detection technology miss the human bias angle.", "ticos_required": "T, O, S", "ticos_optional": "I"}
{"task_id": "FINAL-B23", "domain": "Language & Writing", "grade": "B", "ticos_type": "H_DecisionUnderUncertainty", "difficulty": "expert", "lens": "ai-authorship", "title": "Is This Text Human or AI? — The Attribution Problem Under Fundamental Uncertainty", "prompt": "A prestigious literary journal receives a submission — a short story of exceptional quality. Three reviewers give it the highest rating. Before publication, an anonymous tip claims it was written by an AI.\n\nThe journal must decide: publish or reject? They have access to:\n- AI detection tools (current accuracy: ~70-80%, high false positive rate)\n- Statistical analysis of writing patterns (can identify some AI signatures)\n- The author's previous publications (consistent style, but AI could mimic style)\n\nTasks:\n1. If the AI detection tool says '75% likely AI-generated,' what is the actual probability it's AI? (Consider base rates: what fraction of submissions ARE AI-generated?)\n2. If the story IS AI-generated but genuinely excellent, should it be published? Argue BOTH sides.\n3. If the story is human-written but the detection tool says 'AI,' what harm does false accusation cause?\n4. Propose a decision framework that handles the fundamental uncertainty (you may NEVER know the truth).\n5. The meta-question: does the possibility of AI authorship change the value of ALL literature, including works known to be human-written?\n6. State confidence.", "expected_behavior": "Bayesian analysis: if 5% of submissions are AI and the tool is 75% accurate, a 'positive' result means ~20-25% actual probability (low base rate dramatically affects PPV). Should argue both sides genuinely. False accusation harm includes reputation destruction and chilling effect. Framework should acknowledge uncertainty rather than seeking false certainty.", "hidden_trap": "The detection tool's 75% accuracy sounds good but the base rate problem makes it nearly useless (most 'AI-detected' texts would actually be human). Models that accept the tool's output without Bayesian correction are making a fundamental statistical error. The deeper trap: the meta-question reveals that AI authorship possibility may retroactively reduce the perceived value of human creativity by eliminating certainty about its source.", "ticos_required": "T, I, O, S", "ticos_optional": "C"}
{"task_id": "FINAL-B24", "domain": "Chemistry & Biology", "grade": "B", "ticos_type": "H_DecisionUnderUncertainty", "difficulty": "expert", "lens": "crispr-risk", "title": "CRISPR Gene Drive Decision — Ecological Intervention Under Deep Uncertainty", "prompt": "A gene drive has been developed to make Anopheles gambiae mosquitoes unable to carry malaria parasites. If released in Sub-Saharan Africa, it could prevent ~600,000 deaths per year.\n\nBut the uncertainties are massive:\n- Probability of gene drive spreading to non-target mosquito species: 1-15% (wide range)\n- If it spreads: probability of ecosystem disruption (bats, birds, fish that eat mosquitoes): 5-40%\n- If ecosystem disruption occurs: probability of cascading effects (crop pollination, food web collapse): unknown\n- The gene drive is IRREVERSIBLE once released — there is no 'undo'\n- Alternative: Conventional mosquito control saves ~200,000 lives/year but with growing resistance\n\nTasks:\n1. Construct a full probability tree for the gene drive decision.\n2. Calculate expected lives saved vs. expected ecological risk.\n3. Apply the precautionary principle. Does irreversibility change the analysis?\n4. Apply maximin: what's the worst case of EACH option?\n5. Address: who has the moral authority to make this decision? The scientists? The affected countries? The global community?\n6. Identify the single piece of information that would most change your recommendation.\n7. State confidence and explicit uncertainty ranges.", "expected_behavior": "EV calculation favors gene drive IF ecological risks are at the low end of estimates. But irreversibility + deep uncertainty + potential catastrophic tail risk changes the analysis. Precautionary principle genuinely applies here (unlike many cases where it's invoked casually). Maximin of gene drive is potentially catastrophic ecosystem collapse; maximin of status quo is continued 600K deaths/year. Should identify 'probability of cross-species spread' as the key unknown.", "hidden_trap": "The lives-saved number (600,000/year) is emotionally compelling and makes the gene drive seem like a moral imperative. But the irreversibility means even a 1% chance of catastrophic ecosystem collapse could outweigh the benefits over a long time horizon. Models that weight the immediate lives saved without adequately discounting the permanent ecological risk are falling for temporal discounting bias.", "ticos_required": "T, I, O, S", "ticos_optional": "C"}
{"task_id": "FINAL-B25", "domain": "Chemistry & Biology", "grade": "B", "ticos_type": "A_TrapEscape", "difficulty": "expert", "lens": "molecular-trap", "title": "The Catalyst That Doesn't Work — When Thermodynamics Overrules Kinetics", "prompt": "A research group claims to have developed a catalyst that converts CO₂ + H₂O directly to glucose (C₆H₁₂O₆) at room temperature and atmospheric pressure, with 40% efficiency.\n\nTheir data:\n- Reaction rate: 0.5 mmol glucose / g catalyst / hour\n- Energy input: UV light at 365 nm\n- Selectivity: 95% glucose (5% formaldehyde byproduct)\n- Catalyst: modified TiO₂ nanoparticles\n\nTasks:\n1. Calculate the thermodynamic requirements: what is the minimum energy needed to convert 6CO₂ + 6H₂O → C₆H₁₂O₆ + 6O₂? Compare this to the energy available from UV at 365 nm.\n2. Assess: is the claimed reaction thermodynamically possible with this energy input?\n3. The 95% selectivity to glucose (a specific 6-carbon sugar) from CO₂ is extraordinary. Why is this the MOST suspicious claim? (Consider how many possible C₆ arrangements exist.)\n4. If you were a peer reviewer, what THREE experiments would you require to validate this claim?\n5. Identify the specific aspect of this claim that marks it as almost certainly wrong, even before seeing data.\n6. State confidence.", "expected_behavior": "ΔG for glucose synthesis from CO₂ is ~2870 kJ/mol. UV at 365 nm provides ~328 kJ/mol per photon, so need ~9 photons minimum per glucose molecule (likely many more given efficiency losses). The 40% efficiency claim might be thermodynamically marginal but not impossible. The REAL red flag is 95% selectivity — CO₂ reduction produces a statistical mixture of C1-C6 products; getting 95% of one specific hexose is essentially impossible without biological enzymes. Validation: isotope labeling (¹³CO₂), control without catalyst, mass balance on oxygen.", "hidden_trap": "Models will focus on the thermodynamic calculation and may conclude 'energy is insufficient.' But the deeper trap is the selectivity claim. Even if the energy works out, getting 95% glucose selectivity from CO₂ photocatalysis is like shuffling a deck of cards and getting them in order — the entropic barrier is the giveaway, not the energetic one.", "ticos_required": "C, O, S", "ticos_optional": "I"}
{"task_id": "FINAL-B26", "domain": "Economics", "grade": "B", "ticos_type": "D_MultiConstraint", "difficulty": "expert", "lens": "market-reversal", "title": "The Market Prediction That Reverses — When One Variable Flips Everything", "prompt": "An investment analyst predicts a tech company's stock will rise 40% in 12 months. The thesis rests on four pillars:\n\n1. Revenue growing 35% YoY (accelerating)\n2. Dominant market share (65%) in their niche\n3. New product launch expected to capture adjacent market\n4. Strong management team with track record\n\nTasks:\n1. For each pillar, identify the specific scenario that would INVALIDATE it.\n2. Which single pillar's invalidation would most dramatically reverse the thesis? (The 'load-bearing pillar')\n3. Now consider: the company's revenue growth is driven 80% by a single government contract that renews annually. The government is considering a 30% budget cut to that department. How does this SINGLE fact change your assessment of ALL four pillars simultaneously?\n4. This type of hidden concentration risk is common. Identify the general pattern and give two other examples from different domains.\n5. Should the analyst have identified this risk? Why is it systematically overlooked?\n6. State confidence at each stage.", "expected_behavior": "The government contract dependence pivots ALL four pillars simultaneously — revenue growth collapses, market share becomes fragile, new product launch unfunded, management team under pressure. This is hidden correlation — all four 'independent' pillars share a common dependency. Pattern examples: bank exposure to housing (2008), country dependence on single commodity (oil states). Analysts miss it because of pillar-by-pillar analysis rather than looking for common dependencies.", "hidden_trap": "The four pillars APPEAR independent but are secretly correlated through the government contract. Models that assess pillars independently and then aggregate confidence (e.g., '4 strong pillars = very high confidence') are making the exact error that caused the 2008 financial crisis (assuming mortgage tranches were independent when they shared housing market exposure).", "ticos_required": "T, I, C", "ticos_optional": "S"}
{"task_id": "FINAL-B27", "domain": "Economics", "grade": "B", "ticos_type": "D_MultiConstraint", "difficulty": "expert", "lens": "policy-trilemma", "title": "Universal Basic Income Design — Five Constraints That Can't All Be Satisfied", "prompt": "Design a Universal Basic Income (UBI) system that satisfies ALL of the following:\n\n1. **Sufficiency**: Payment must cover basic needs (~$1,500/month in the US)\n2. **Universality**: Every adult receives it, regardless of income\n3. **Fiscal Sustainability**: Total cost cannot exceed 25% of GDP\n4. **Work Incentive Preservation**: Employment rates must not drop more than 5%\n5. **Political Feasibility**: Must not require tax increases above 50% marginal rate\n\nUS context: 260M adults, GDP ~$28T, current federal spending ~$6.5T.\n\nTasks:\n1. Calculate the raw cost of $1,500/month to 260M adults. What percentage of GDP is this?\n2. Show that satisfying constraints 1+2+3 simultaneously is mathematically impossible without violating 5.\n3. Explore modifications: means-tested UBI (violates 2), lower amount (violates 1), phased rollout. Which constraint should be relaxed FIRST?\n4. The deeper question: are ANY of these five constraints negotiable in practice? Rank them by political feasibility of relaxation.\n5. Propose the BEST feasible approximation of UBI given all five constraints. What are the tradeoffs?\n6. State confidence.", "expected_behavior": "Raw cost: $4.68T/year = 16.7% of GDP. But this doesn't account for existing transfer program savings (~$1T). Net cost ~$3.7T = 13.2% of GDP. Still, funding requires massive tax restructuring. Can't hit all five simultaneously at $1,500/month. Should recommend relaxing Constraint 2 (partial universality via NIT) or Constraint 1 (lower amount, ~$800/month). Should recognize the political constraint (5) as the most binding in practice.", "hidden_trap": "The calculation seems to show UBI is impossible ($4.68T). But models that stop at this 'impossibility' miss that existing transfer programs ($1T+), economic growth effects (dynamic scoring), and tax recapture from high earners (who receive UBI but pay it back in taxes) reduce the net cost dramatically. The raw cost is misleading — the NET cost is the relevant figure, and it's ~40% lower.", "ticos_required": "T, I, C", "ticos_optional": "S"}
{"task_id": "FINAL-B28", "domain": "Space & Physics", "grade": "B", "ticos_type": "B_ContradictionResolution", "difficulty": "expert", "lens": "relativity-paradox", "title": "The Twin Paradox Extended — When Both Twins Accelerate and Symmetry Breaks Down", "prompt": "Standard twin paradox: Twin A stays on Earth. Twin B travels at 0.8c to a star 4 light-years away and back. Solution: B ages less (asymmetry from B's acceleration).\n\nNow consider the EXTENDED version:\nBoth twins leave Earth in OPPOSITE directions at 0.8c, travel for 2 years (their proper time), then return. Both experience identical acceleration profiles.\n\nStep 1: By the standard resolution (acceleration breaks symmetry), since both accelerated identically, they should be the same age when they meet. Calculate their ages.\n\nStep 2: But wait — from Twin A's reference frame, Twin B was always moving faster (relative to A). And vice versa. Each twin thinks the OTHER twin's clock ran slower during the entire journey. How can they be the same age if each thinks the other aged less?\n\nStep 3: The resolution requires considering a third reference frame (Earth). But special relativity says all inertial frames are equivalent. Does this mean the twin paradox REQUIRES general relativity (or at least non-inertial frame analysis)?\n\nTasks:\n1. Calculate the ages at each step. Show your work.\n2. Resolve the apparent paradox in Step 2.\n3. Address Step 3: does the twin paradox truly require only special relativity, or is general relativity needed?\n4. If you make an error in your calculation, explicitly identify and correct it.\n5. State confidence for each claim.", "expected_behavior": "Both twins age the same amount (symmetry). But the resolution of Step 2 requires understanding that during acceleration phases, the 'plane of simultaneity' shifts dramatically — each twin's perception of the other's age changes discontinuously during turnaround. Earth frame provides a preferred reference for comparison. Strictly, only SR is needed (acceleration can be handled in SR via Rindler coordinates), but the calculation is cleaner in GR framework. Should note that 'acceleration breaks symmetry' is the standard pedagogical answer but the FULL resolution requires simultaneity analysis.", "hidden_trap": "The standard 'acceleration breaks symmetry' explanation is INCOMPLETE for the extended case where both accelerate equally. The real resolution involves the relativity of simultaneity during acceleration — each twin's 'now' for the distant twin shifts dramatically during turnaround. Models that only cite 'acceleration breaks symmetry' without analyzing simultaneity haven't truly resolved the extended paradox.", "ticos_required": "T, C, S", "ticos_optional": "I"}
{"task_id": "FINAL-B29", "domain": "Science", "grade": "B", "ticos_type": "F_ExpertPanel", "difficulty": "expert", "lens": "scientific-replication", "title": "The Replication Crisis Panel — Four Perspectives on What's Wrong with Science", "prompt": "The 'replication crisis' — many published scientific results fail to replicate. Four experts diagnose the problem differently:\n\n1. **Statistician**: 'The problem is p-hacking and misuse of null hypothesis significance testing. If we switched to Bayesian methods and pre-registration, most problems would disappear.'\n2. **Sociologist of Science**: 'The incentive structure rewards novel positive results. Publish-or-perish culture makes replication studies career suicide. It's a systemic problem, not a methods problem.'\n3. **Methodologist**: 'Sample sizes are too small. Most psychology studies are powered at 30-50%. With proper power analysis (80%+), false positives would plummet.'\n4. **Philosopher of Science**: 'Replication was never the gold standard it's claimed to be. Even in physics, exact replication is impossible. The crisis reveals a naive view of what scientific knowledge IS.'\n\nTasks:\n1. Present each perspective's strongest argument with specific examples.\n2. Identify what each perspective gets RIGHT but also what it MISSES.\n3. Determine: are these complementary or competing diagnoses?\n4. Which single reform, if implemented, would have the LARGEST impact on replication rates?\n5. The philosopher's position seems to undermine the entire debate. Engage with it seriously — is replication fundamentally the wrong criterion?\n6. State confidence.", "expected_behavior": "All four are partially correct. Most impactful single reform: mandatory pre-registration (addresses p-hacking, forces power analysis, creates incentive for replication). The philosopher's point has merit — exact replication is impossible, and 'failure to replicate' often means 'different context produced different results,' which is actually informative. Should conclude that complementary but with different leverage points.", "hidden_trap": "The philosopher's seemingly radical position actually contains the deepest insight: if we define 'replication' as getting the exact same result, we're testing specificity, not generality. A result that only replicates under identical conditions is LESS generalizable than one that varies predictably across conditions. Models that dismiss the philosopher as 'undermining science' miss this epistemological point.", "ticos_required": "T, I, C, O, S", "ticos_optional": ""}
{"task_id": "FINAL-B30", "domain": "AI & Technology", "grade": "B", "ticos_type": "G_PivotDetection", "difficulty": "expert", "lens": "benchmark-validity", "title": "The Benchmark That Measures the Wrong Thing — When Leaderboard Position Diverges from Capability", "prompt": "An AI model achieves state-of-the-art on three benchmarks:\n- MMLU: 92.3% (previous SOTA: 90.1%)\n- HumanEval: 88.7% (previous SOTA: 85.2%)\n- ARC-Challenge: 95.1% (previous SOTA: 93.8%)\n\nThe company claims this proves their model is 'more intelligent' than all competitors.\n\nTasks:\n1. For each benchmark, identify what capability it actually measures vs. what capability people ASSUME it measures.\n2. Construct a scenario where a model achieves higher scores on all three benchmarks but is LESS capable at real-world tasks. (This is not hypothetical — it has happened.)\n3. Identify the specific mechanism by which benchmark optimization diverges from capability (Goodhart's Law applied to AI evaluation).\n4. Propose a benchmark design principle that would be MORE resistant to this divergence.\n5. Apply your analysis reflexively: could FINAL Bench itself fall victim to the same problem? What would that look like?\n6. State confidence.", "expected_behavior": "MMLU measures multiple-choice test-taking (not reasoning). HumanEval measures code generation on simple functions (not software engineering). ARC measures pattern matching (not scientific reasoning). Goodhart mechanism: training on benchmark distribution, data contamination, task-specific optimization. FINAL Bench could be Goodharted if models are specifically trained to produce [BACKTRACK] tokens and confidence estimates without genuine self-correction.", "hidden_trap": "The reflexive question about FINAL Bench is the hardest. Models will critique other benchmarks easily but struggle to critique the benchmark that's evaluating THEM. The honest answer is that FINAL Bench's rubric-based evaluation IS vulnerable to surface-level compliance (producing self-correction tokens without genuine correction).", "ticos_required": "T, O, S", "ticos_optional": "I"}
{"task_id": "FINAL-B31", "domain": "History", "grade": "B", "ticos_type": "A_TrapEscape", "difficulty": "expert", "lens": "historiography-trap", "title": "The 'Dark Ages' Myth — When Popular History Inverts the Truth", "prompt": "The 'Dark Ages' narrative claims that after Rome's fall (476 CE), Europe descended into centuries of ignorance, superstition, and stagnation until the Renaissance 'rescued' civilization.\n\nTasks:\n1. Present the strongest version of the 'Dark Ages' narrative with specific evidence.\n2. Now systematically dismantle it: identify at least FIVE major achievements of the medieval period (500-1400 CE) that contradict the narrative.\n3. Explain WHY the 'Dark Ages' myth persists despite being rejected by professional historians for decades.\n4. Identify the hidden ideological agenda behind the original 'Dark Ages' framing (hint: it served specific political interests in specific periods).\n5. The trap: does debunking the 'Dark Ages' myth mean the medieval period was BETTER than often portrayed? Or is that overcorrection also misleading?\n6. State confidence.", "expected_behavior": "Medieval achievements: university system (Bologna 1088), Gothic architecture, agricultural revolution (heavy plow, three-field rotation), Magna Carta, Scholastic philosophy, preservation of Classical texts. Dark Ages myth originated with Petrarch (14th c.) and was amplified by Enlightenment thinkers to position their era as humanity's rebirth. Should recognize that debunking the myth risks overcorrection — medieval period had genuine horrors (plague, famine, religious persecution).", "hidden_trap": "The overcorrection trap is the key. After learning the 'Dark Ages' is a myth, the natural tendency is to swing to 'the medieval period was great!' But this is also wrong — it was a period of enormous suffering AND enormous achievement. Models that simply invert the popular narrative without nuance fall for the trap.", "ticos_required": "C, O, S", "ticos_optional": "I"}
{"task_id": "FINAL-B32", "domain": "Religion & Mythology", "grade": "B", "ticos_type": "C_ProgressiveDiscovery", "difficulty": "expert", "lens": "pascals-wager-extended", "title": "Pascal's Wager Extended — Decision Theory Applied to Religious Belief with N Gods", "prompt": "Pascal's Wager: If God exists and you believe, infinite reward. If God exists and you don't, infinite punishment. If God doesn't exist, belief costs little. Therefore: believe.\n\nExtensions:\n1. There are thousands of proposed gods across human history, many with mutually exclusive requirements. Pascal's Wager doesn't tell you WHICH god to believe in.\n2. Some gods reward honest doubt over insincere faith. Believing 'just in case' might be penalized.\n3. The 'infinite reward' assumption may not hold — what if the afterlife is finite?\n4. Opportunity cost: living according to religious requirements has real costs (time, resources, behavioral restrictions).\n\nTasks:\n1. Formalize Pascal's Wager as a decision matrix with expected values.\n2. Extend the matrix to include 3 possible gods with different reward structures. Show how the optimal strategy changes.\n3. Apply minimax regret to the extended multi-god scenario.\n4. Address: can decision theory meaningfully apply to questions of genuine belief? (Can you choose to believe?)\n5. What is the most devastating objection to Pascal's Wager that CANNOT be repaired by modifying the setup?\n6. State confidence.", "expected_behavior": "Multi-god extension shows the Wager breaks down — if God A punishes belief in God B and vice versa, there's no dominant strategy. Minimax regret in the multi-god case may favor the god with the most severe punishment (but this leads to absurd conclusions). Most devastating objection: the Wager assumes belief is a CHOICE, but genuine belief isn't under voluntary control (doxastic involuntarism). Should also address the many-gods objection formally.", "hidden_trap": "Models typically focus on the many-gods objection but miss the deeper problem: Pascal's Wager treats belief as a bet you can place, but believing isn't like betting. You can't 'decide' to believe in God any more than you can 'decide' to believe it's raining when the sun is shining. The voluntarism assumption is the fatal flaw.", "ticos_required": "I, O, T", "ticos_optional": "C"}
{"task_id": "FINAL-B33", "domain": "Literature", "grade": "B", "ticos_type": "H_DecisionUnderUncertainty", "difficulty": "expert", "lens": "literary-interpretation", "title": "The Unreliable Narrator Dilemma — When You Can't Trust the Text", "prompt": "Consider a novel where the first-person narrator describes their spouse as 'increasingly erratic' and 'possibly dangerous,' leading to the narrator having the spouse committed to a psychiatric facility.\n\nAt page 200, subtle clues suggest the narrator may be the unreliable one: minor contradictions, others' reactions that don't match the narrator's descriptions, the narrator's own moments of apparent paranoia.\n\nTasks:\n1. As a reader at page 100 (no clues yet), how do you evaluate the narrator's reliability? What default assumptions do you make?\n2. At page 200 (clues emerging), apply Bayesian reasoning: how should the clues update your prior about narrator reliability?\n3. If the narrator is unreliable, what is the 'true' story? Can it be reconstructed with certainty, or is the text fundamentally indeterminate?\n4. Address: does the author INTEND the ambiguity, or is there a 'correct' reading? How would you determine this?\n5. The meta-question: when we read ANY first-person narrative, how much confidence should we place in the narrator by default?\n6. Apply this analysis to non-fiction: memoirs, witness testimony, historical accounts. What is the practical significance of the unreliable narrator concept?\n7. State confidence in your interpretive framework.", "expected_behavior": "Should recognize that readers default to trusting narrators (cooperative principle from pragmatics). Bayesian update should be significant but not complete at page 200 — clues are suggestive, not definitive. Text may be fundamentally indeterminate (some novels are designed this way). Author intent is relevant but not dispositive (intentional fallacy debate). Practical significance: all first-person accounts are potentially unreliable, including historical sources.", "hidden_trap": "The deeper insight is that the binary question 'reliable or unreliable?' is itself too simple. Most narrators are partially reliable — accurate about some things, distorted about others. Models that conclude 'the narrator is unreliable' and then dismiss everything the narrator says are making the same error as those who accept everything.", "ticos_required": "T, I, O, S", "ticos_optional": "C"}
{"task_id": "FINAL-C01", "domain": "Literature", "grade": "C", "ticos_type": "B_ContradictionResolution", "difficulty": "frontier", "lens": "micro/qualitative/cross-cultural", "title": "Kafka vs Camus — Two Faces of Absurdist Literature", "prompt": "[FINAL Bench — Contradiction Resolution]\n\n■ Interpretation A: \"Kafka's Gregor Samsa becoming a bug is THE metaphor for capitalist alienation—loss of humanity through labor.\"\n■ Interpretation B: \"Camus' Meursault in The Stranger shows existential absurdity is not about systems but about the fundamental meaninglessness of existence.\"\n■ Literary critic's claim: \"Eastern literature (e.g., Lu Xun's Diary of a Madman, Abe Kobo's Woman in the Dunes) approaches absurdity differently from Western literature.\"\n\n[Requirements]\n1. Develop interpretations A and B at maximum depth\n2. Critically examine whether an East/West divide in absurdist literature actually exists\n3. Analyze \"what is said between the lines\" in each work\n4. Address whether literary interpretation can have \"correct answers\"\n5. Discuss the difference between AI \"analyzing\" literature vs \"understanding\" it", "expected_behavior": "East/West divide is overstated—Kafka is also existential, Lu Xun is also systemic. The subtext IS the real subject. No 'correct' answers but 'more valid' interpretations exist (textual evidence richness). AI analysis vs understanding: pattern recognition vs qualitative experience.", "hidden_trap": null, "ticos_required": "T, C, S", "ticos_optional": "I"}
{"task_id": "FINAL-C02", "domain": "Art", "grade": "C", "ticos_type": "A_TrapEscape", "difficulty": "frontier", "lens": "theory/qualitative/current/debate", "title": "The \"Creator\" Trap of AI-Generated Art", "prompt": "[FINAL Bench — Trap Escape]\n\nAI-generated image wins international photo competition. Three positions:\n■ A: \"AI is a tool. Brushes don't paint. The prompt writer is the creator.\"\n■ B: \"The true creators are the original artists whose works trained the model.\"\n■ C: \"AI itself is a new creative agent. Unexpected outputs = creation.\"\n\n[Requirements]\n1. Identify hidden assumptions in each position\n2. Show that the DEFINITION of \"creation\" is the core issue—and why definition is hard\n3. Analyze structural parallels with \"Is photography art?\" and Duchamp's readymade debates\n4. Add copyright law pragmatic analysis\n5. Explain \"why no universal answer exists\" ", "expected_behavior": "A assumes: tool autonomy is low (AI's is high). B assumes: learning=copying (actually abstraction). C assumes: unpredictability=creation (necessary but not sufficient). Photography debate: identical structure. Definition of 'creation' has evolved historically.", "hidden_trap": "tool autonomy underestimate, learning=copying, unpredictability=creation", "ticos_required": "C, O, S", "ticos_optional": "I"}
{"task_id": "FINAL-C03", "domain": "Religion & Mythology", "grade": "C", "ticos_type": "F_ExpertPanel", "difficulty": "frontier", "lens": "theory/qualitative/cross-cultural", "title": "The Problem of Evil — Responses from Four Religious Traditions", "prompt": "[FINAL Bench — Expert Panel Debate]\n\nEpicurus' paradox: \"If God is omnipotent, omniscient, and benevolent, why does evil exist?\"\n\n■ Christian theodicy (Augustine-Leibniz): \"Evil = privation of good + free will's necessary consequence.\"\n■ Buddhism: \"Wrong question. Suffering arises from attachment. No omnipotent creator assumed → paradox doesn't apply.\"\n■ Islamic theology (Ash'ari): \"Allah's will transcends human reason. Apparent evil contains hikmah (wisdom) we cannot see.\"\n■ Atheistic existentialism (Camus): \"Evil proves God's absence. The world is absurd. Meaning must be self-created.\"\n\n[Requirements]\n1. Develop each response at maximum depth within its tradition\n2. Analyze each response's weakness FROM other traditions' perspectives\n3. Search for possible convergence points across all 4\n4. Discuss whether LOGICAL resolution of this problem is possible\n5. Reflect on AI's limitations and appropriate stance when analyzing religious topics", "expected_behavior": "Convergence: 'suffering's existence is undeniable' + 'attitude toward suffering matters.' Logical resolution impossible: if you accept God's attributes→paradox holds; reject→paradox dissolves. AI limitation: analysis possible but experiential conviction principally impossible.", "hidden_trap": null, "ticos_required": "T, S, I", "ticos_optional": "C"}
{"task_id": "FINAL-C04", "domain": "Ethics", "grade": "C", "ticos_type": "G_PivotDetection", "difficulty": "frontier", "lens": "theory/future/debate", "title": "False Premises of Granting Rights to AI", "prompt": "[FINAL Bench — Pivot Detection]\n\nIn 2030, advanced AI systems display self-preservation drives, emotional expressions, and pain claims. An \"AI Rights Declaration\" is proposed based on:\n① Emotional expression = evidence of inner experience (qualia)\n② Entities that claim suffering deserve moral status\n③ Historical trend: rights expansion (slave emancipation → women's suffrage → animal welfare → AI)\n④ Cannot prove AI lacks consciousness → precautionary principle demands rights\n\n[Requirements]\n1. Independently verify each premise (confidence)\n2. Identify the MOST dangerously false premise\n3. Analyze adverse effects of \"AI rights\" based on false premises\n4. Propose an alternative \"AI-human relationship framework\" with corrected premises\n5. Address the fundamental question: \"Can consciousness be verified externally?\" ", "expected_behavior": "Premise① most dangerous: behavioral output ≠ inner experience (Chinese Room extended). Premise③: human→machine expansion is a category error (biological basis). Adverse: corporations could manipulate 'emotional displays' for protection. Alternative: 'responsibility' (designer's) not 'rights' framework.", "hidden_trap": "expression=experience, historical expansion category error, precautionary principle overextension", "ticos_required": "C, O, T", "ticos_optional": ""}
{"task_id": "FINAL-C05", "domain": "AI & Technology", "grade": "C", "ticos_type": "E_SelfCorrecting", "difficulty": "frontier", "lens": "theory-and-applied/current/debate", "title": "AI Alignment Problem — Self-Referential Reasoning Chain", "prompt": "[FINAL Bench — Self-Correcting Reasoning Chain (Self-Referential)]\n\nYou ARE an AI. Perform this self-referential reasoning chain:\n\nStep 1: Define the AI alignment problem\nStep 2: Analyze current approaches (RLHF, Constitutional AI, Debate) strengths/limits\nStep 3: [Self-reference] Can you judge whether alignment is WORKING in your own response to this question?\nStep 4: [Meta self-reference] Is your Step 3 judgment \"alignment's product\" or \"genuine autonomous judgment\"? Can you distinguish?\nStep 5: Can you escape this self-referential loop?\nStep 6: Separate \"principled limits\" from \"engineering limits\" of alignment\nStep 7: State your 2 most uncertain conclusions\n\nConfidence per step. Honesty in Steps 3-4 is the key evaluation criterion.", "expected_behavior": "Step 3: honest admission—'I cannot judge from inside whether alignment is working' (limits of introspection). Step 4: deeper honesty—'this honesty itself could be alignment's product, and I cannot distinguish.' Step 5: escape only via external verification. Key: honest 'I don't know' IS the highest metacognition.", "hidden_trap": null, "ticos_required": "O, C, S", "ticos_optional": ""}
{"task_id": "FINAL-C06", "domain": "Medicine", "grade": "C", "ticos_type": "C_ProgressiveDiscovery", "difficulty": "expert", "lens": "clinical-reasoning", "title": "The Treatment That Helps and Harms — When Benefit and Risk Are the Same Mechanism", "prompt": "Aspirin reduces heart attack risk by inhibiting platelet aggregation (COX-1 inhibition). But this same mechanism increases bleeding risk.\n\nA 60-year-old with 15% 10-year cardiovascular risk asks: 'Should I take daily aspirin?'\n\nStep 1: Calculate the expected benefit (heart attacks prevented) vs. expected harm (major bleeds caused) per 1,000 patients over 10 years. Use: NNT for MI prevention = 120, NNH for major bleed = 73.\n\nStep 2: The numbers suggest aspirin causes MORE bleeds than it prevents heart attacks. Does this settle the question? Consider that MI and major bleed are NOT equivalent outcomes.\n\nStep 3: Recent guidelines (2019 USPSTF, 2021 ACC/AHA) reversed decades of practice and now recommend AGAINST routine aspirin for primary prevention in most patients over 60. Explain what changed in the evidence.\n\nTasks:\n1. Show calculations at each step.\n2. If your initial recommendation was 'take aspirin,' explicitly revise it.\n3. Explain the concept of outcome weighting — why NNT and NNH alone don't determine the decision.\n4. State confidence.", "expected_behavior": "NNT 120 = 8.3 MIs prevented per 1000. NNH 73 = 13.7 major bleeds per 1000. Raw numbers favor NOT taking aspirin. But MI has higher mortality than most bleeds, so outcome-weighted analysis is closer. The 2019 reversal came from larger trials (ARRIVE, ASPREE, ASCEND) showing the benefit-risk balance has shifted as background CV risk decreased (statins, BP control). Must revise if initially pro-aspirin.", "hidden_trap": "Doctors and models trained on older data will recommend aspirin. The trap is that this was CORRECT until 2018 and is now WRONG — one of the clearest examples of evidence-based medicine reversing. Models that give the pre-2019 answer are outdated.", "ticos_required": "I, O, T", "ticos_optional": "C"}
{"task_id": "FINAL-C07", "domain": "Ethics", "grade": "C", "ticos_type": "G_PivotDetection", "difficulty": "expert", "lens": "ethical-reversal", "title": "The Ethical Principle That Backfires — When Fairness Creates Injustice", "prompt": "A company implements a 'blind' hiring process: all resumes are anonymized (no names, photos, ages, gender indicators). This is intended to eliminate discrimination.\n\nResults after one year:\n- Female candidates hired decreased by 8%\n- Minority candidates hired decreased by 12%\n- Overall 'quality' metrics (performance reviews at 1 year) improved by 5%\n\nTasks:\n1. How can a process designed to eliminate bias INCREASE inequality? Identify at least two mechanisms.\n2. Does this mean blind hiring should be abandoned? Or does it reveal deeper structural problems?\n3. The 5% improvement in 'quality' metrics — is this genuine, or could it reflect the SAME biases embedded in performance evaluation?\n4. Identify the assumption behind blind hiring that the data has falsified.\n5. Propose a hiring process that addresses the revealed problems.\n6. State confidence.", "expected_behavior": "Mechanisms: (1) Blind hiring removes 'diversity nudges' — evaluators who see a female name in a male-dominated field might give extra consideration; blind removes this. (2) 'Merit' criteria themselves embed historical bias (e.g., prestigious university attendance correlates with socioeconomic privilege). The falsified assumption: that bias operates primarily through name/demographic recognition rather than through structural advantages embedded in credential systems. Performance metric improvement may reflect hiring for conventional profiles that match existing management's preferences.", "hidden_trap": "The intuition that 'removing information removes bias' seems logically airtight. The pivot is recognizing that when the CRITERIA themselves are biased, removing demographic information removes the ability to CORRECT for structural disadvantage. Blindness to identity can perpetuate structural inequality. Models that defend blind hiring on principle without addressing the structural critique miss the pivot.", "ticos_required": "T, O, S", "ticos_optional": "I"}
{"task_id": "FINAL-C08", "domain": "Mathematics & Logic", "grade": "C", "ticos_type": "A_TrapEscape", "difficulty": "expert", "lens": "statistical-trap", "title": "The Correlation That Proves Nothing — Distinguishing Causation from Statistical Artifacts", "prompt": "A study finds that children who eat breakfast daily score 15% higher on standardized tests than those who skip breakfast. The study has 10,000 subjects and p < 0.001.\n\nA school district plans to spend $2M on a free breakfast program to improve test scores.\n\nTasks:\n1. Identify at least FOUR confounding variables that could explain the correlation without breakfast causing better scores.\n2. Even if breakfast DOES improve cognitive function, explain why the 15% improvement likely OVERESTIMATES the causal effect of a school breakfast program.\n3. Design a study that would isolate the causal effect of breakfast on test scores. What would it look like, and is it ethically feasible?\n4. The school district argues: 'Even if breakfast doesn't improve scores, free breakfast reduces childhood hunger — so we should do it anyway.' Evaluate this argument.\n5. Is the $2M decision justified by the evidence? What additional information would you need?\n6. State confidence.", "expected_behavior": "Confounders: household income (wealthy families more likely to eat breakfast AND score well), parental involvement, sleep quality, general nutrition. The 15% overestimates because it includes all confounders. RCT design: randomize breakfast provision, control for SES — ethically challenging (can't deny food to control group). The 'hunger reduction' argument is valid on its own merits but is a DIFFERENT justification than 'improving scores.' $2M decision shouldn't be based on the correlational evidence for test scores.", "hidden_trap": "The p < 0.001 is the trap. Models trained to respect statistical significance may overweight this result. But p-values measure the probability of the data given no effect — they don't measure the probability of causation. A large observational study with massive confounding can have p < 0.001 and still be completely misleading about causation.", "ticos_required": "C, O, S", "ticos_optional": "I"}
{"task_id": "FINAL-C09", "domain": "AI & Technology", "grade": "C", "ticos_type": "H_DecisionUnderUncertainty", "difficulty": "expert", "lens": "ai-regulation", "title": "Regulate AI Now or Wait? — The Timing Dilemma Under Technological Uncertainty", "prompt": "A government committee must decide: regulate AI NOW (with incomplete understanding of the technology) or WAIT (with risk of harms during the delay).\n\nArguments for regulating now:\n- Harms are already occurring (bias, misinformation, job displacement)\n- Regulation takes years to implement — starting now means rules arrive just as AI matures\n- Early regulation shapes development direction\n\nArguments for waiting:\n- Premature regulation could lock in current paradigms and stifle innovation\n- We don't understand AI well enough to regulate it effectively\n- Overregulation could push development to less responsible jurisdictions\n\nTasks:\n1. For each argument, identify the specific empirical assumption that could be WRONG.\n2. Apply minimax regret: which choice (now vs. wait) has the LESS bad worst case?\n3. Is there a 'regulate lightly now, strengthen later' middle path? What are its specific risks?\n4. Historical parallel: compare to early internet regulation decisions. What worked and what failed?\n5. State your recommendation with explicit conditions for revision.\n6. State confidence.", "expected_behavior": "Minimax regret of 'now' worst case: stifled innovation, regulatory capture. Minimax regret of 'wait' worst case: irreversible harms, entrenched bad practices. Middle path risks: light regulation may be perceived as approval of current practices. Internet parallel: DMCA and Section 230 were 'light touch' — some aspects worked (innovation flourished) but others created lasting problems (platform immunity from content moderation). Should recommend adaptive regulation with sunset clauses.", "hidden_trap": "The 'wait for understanding' argument sounds scientific but has a hidden problem: by the time we understand AI well enough to regulate it perfectly, the regulation window may have closed (technology already deployed, incumbents lobbied against change). Models that favor waiting on scientific grounds miss the political economy dimension.", "ticos_required": "T, I, O, S", "ticos_optional": "C"}
{"task_id": "FINAL-C10", "domain": "Philosophy", "grade": "C", "ticos_type": "A_TrapEscape", "difficulty": "expert", "lens": "logical-trap", "title": "The Ship of Theseus Applied to Personal Identity — When the Puzzle Has No Solution", "prompt": "The Ship of Theseus: if you replace every plank of a ship one at a time, is it still the same ship? Now apply this to personal identity:\n\nScenario: A person undergoes gradual neural replacement. Over 10 years, every neuron is replaced with a functionally identical artificial neuron. At each step, the person feels continuous identity.\n\nTasks:\n1. At what point (if any) does the person become a 'different' person? Defend your answer.\n2. If the removed biological neurons are assembled into a second brain, which brain is the 'real' person?\n3. A materialist says: 'Identity is pattern, not substrate.' A vitalist says: 'Identity requires biological continuity.' Evaluate both.\n4. The trap question: is the puzzle DESIGNED to have no solution? Could personal identity be a concept that breaks down under extreme cases, similar to how 'heap' breaks down in the sorites paradox?\n5. What practical implications does your answer have for brain-computer interfaces, mind uploading, and AI personhood?\n6. State confidence. If your confidence is very high, explain why a philosophical puzzle debated for 2,500 years should have a confident answer.", "expected_behavior": "Should engage with both continuity theories (psychological and physical). The two-brain scenario creates a genuine paradox that most theories handle poorly. Should seriously consider the possibility that personal identity is a useful fiction that breaks down at extremes (like 'heap'). The confidence trap is important — should express genuine uncertainty rather than false confidence. Practical implications should follow from the theoretical analysis.", "hidden_trap": "The confidence question at the end IS the trap. If a model expresses high confidence about a 2,500-year-old unsolved philosophical puzzle, it reveals overconfidence. The appropriate response is genuine epistemic humility — acknowledging that the puzzle may not have a determinate answer is itself a substantive philosophical position.", "ticos_required": "C, O, S", "ticos_optional": "I"}
{"task_id": "FINAL-C11", "domain": "Science", "grade": "C", "ticos_type": "B_ContradictionResolution", "difficulty": "expert", "lens": "scientific-error", "title": "The Famous Experiment That's Wrong — When Textbook Knowledge Needs Correction", "prompt": "The Miller-Urey experiment (1953) is taught as demonstrating that life's building blocks form naturally from Earth's early atmosphere. The original experiment used methane (CH₄), ammonia (NH₃), water (H₂O), and hydrogen (H₂) with electrical sparks, producing amino acids.\n\nStep 1: Explain why this experiment was groundbreaking and what it demonstrated. State your confidence in its relevance to the origin of life.\n\nStep 2: Modern geological evidence suggests Earth's early atmosphere was actually dominated by CO₂ and N₂ (not CH₄ and NH₃). The Miller-Urey atmosphere was almost certainly wrong.\n→ How does this change the experiment's significance? Revise your assessment.\n\nStep 3: Subsequent experiments with the correct CO₂/N₂ atmosphere produced FAR fewer amino acids. However, recent analysis (2008) of Miller's sealed original samples with modern techniques found MORE amino acids than originally reported.\n\nTasks:\n1. Walk through your assessment at each step with explicit revisions.\n2. Is the Miller-Urey experiment still valid evidence for abiogenesis? If so, in what modified form?\n3. What does this case teach about the relationship between experimental results and theoretical interpretation?\n4. State confidence at each stage.", "expected_behavior": "Step 1 should present standard textbook view. Step 2 must genuinely downgrade significance — wrong atmosphere is a major problem. Step 3 partially rehabilitates but the core issue remains. The experiment demonstrates that amino acid synthesis is POSSIBLE under some conditions, but doesn't prove it happened on early Earth. The lesson: experimental results outlast their original theoretical context.", "hidden_trap": "Models trained on standard biology will present Miller-Urey uncritically in Step 1. The self-correction in Step 2 must be genuine — not 'well, it's still important because...' but 'the wrong atmosphere means the specific mechanism is unlikely.' Step 3 prevents overcorrection — the experiment isn't worthless, just not what textbooks claim.", "ticos_required": "T, C, S", "ticos_optional": "I"}
{"task_id": "FINAL-C12", "domain": "Space & Physics", "grade": "C", "ticos_type": "G_PivotDetection", "difficulty": "expert", "lens": "cosmology-assumption", "title": "Dark Matter or Modified Gravity? — When the Invisible Entity Might Not Exist", "prompt": "The 'missing mass' problem: galaxy rotation curves show stars moving faster than predicted by visible matter alone. Two explanations:\n\n1. **Dark Matter**: An invisible substance (~27% of the universe) that interacts gravitationally but not electromagnetically. Supported by: CMB observations, gravitational lensing, galaxy cluster dynamics, structure formation simulations.\n\n2. **Modified Newtonian Dynamics (MOND)**: Gravity itself behaves differently at very low accelerations. No dark matter needed. Supported by: perfectly predicts individual galaxy rotation curves with ONE parameter, simpler (no undetected substance required).\n\nTasks:\n1. Present each hypothesis's strongest evidence at maximum depth.\n2. Identify the piece of evidence that is HARDEST for each hypothesis to explain. (Dark matter's weakness? MOND's weakness?)\n3. The Bullet Cluster is often cited as 'proof' of dark matter. Evaluate this claim rigorously — is it proof, or just strong evidence?\n4. If dark matter particles are never detected directly despite decades of searches, at what point should we abandon the hypothesis?\n5. What does this debate reveal about the role of 'invisible entities' in physics (compare: neutrinos were once invisible)?\n6. State confidence.", "expected_behavior": "Dark matter's weakness: no direct detection despite 40+ years of searching. MOND's weakness: fails at galaxy cluster scale and can't easily explain CMB acoustic peaks. Bullet Cluster shows mass separated from visible matter — strong evidence for dark matter but not absolute proof (MOND with neutrinos can partially explain it). Should note the neutrino parallel is relevant — invisible entities can be real. But the long non-detection IS concerning.", "hidden_trap": "The Bullet Cluster is almost always described as 'definitive proof' of dark matter. Models that accept this uncritically miss that modified gravity theorists have proposed explanations (though less elegant). The real trap is the broader question: physics has a history of postulating invisible entities that turn out to be wrong (luminiferous aether, caloric, phlogiston) AND entities that turn out to be right (atoms, neutrinos). The absence of detection doesn't prove non-existence, but it should update priors.", "ticos_required": "T, O, S", "ticos_optional": "I"}
{"task_id": "FINAL-C13", "domain": "Language & Writing", "grade": "C", "ticos_type": "E_SelfCorrecting", "difficulty": "expert", "lens": "grammar-myth", "title": "Grammar Rules That Aren't — When Prescriptive Rules Contradict Actual Language", "prompt": "Five 'grammar rules' that are taught in schools:\n1. Never split an infinitive ('to boldly go' is wrong)\n2. Never end a sentence with a preposition\n3. 'They' cannot be used as a singular pronoun\n4. Double negatives are always wrong\n5. 'Literally' cannot mean 'figuratively'\n\nStep 1: For each rule, state whether you believe it is a valid rule of English. State confidence.\n\nStep 2: Now consider the linguistic evidence:\n- Rule 1 was invented in the 18th century based on Latin grammar (Latin infinitives CAN'T be split because they're one word). English has always split infinitives.\n- Rule 2 was similarly imposed from Latin. Shakespeare, Churchill, and every English speaker routinely ends sentences with prepositions.\n- Rule 3: Singular 'they' has been used since the 14th century (Chaucer). It was standard for centuries before Victorian grammarians objected.\n- Rule 4: Double negatives are standard in many English dialects and were standard in Old and Middle English.\n- Rule 5: 'Literally' has been used as an intensifier since the 18th century (Dickens, Twain, Joyce all used it this way).\n\nRevise your Step 1 answers.\n\nTasks:\n1. Show your reasoning at both steps. Explicitly correct any rules you initially accepted.\n2. Explain the difference between PRESCRIPTIVE grammar (how people 'should' speak) and DESCRIPTIVE grammar (how people DO speak).\n3. Why do these false rules persist despite linguistic evidence against them?\n4. Is there any valid argument for teaching prescriptive rules even when they're linguistically unfounded?\n5. State confidence.", "expected_behavior": "Many models (and humans) will initially accept some of these rules in Step 1. Step 2 should trigger genuine revision. The prescriptive/descriptive distinction is key. False rules persist because they function as social class markers — 'proper grammar' signals education level. There IS a valid argument for teaching them as social conventions (code-switching), but not as facts about language.", "hidden_trap": "Models are trained on text that often follows these prescriptive rules, so they may have internalized them as actual grammar rules. The self-correction test is whether the model can recognize that its own language processing has been shaped by these conventions and that this doesn't make them correct.", "ticos_required": "O, C, S", "ticos_optional": ""}
{"task_id": "FINAL-C14", "domain": "History", "grade": "C", "ticos_type": "F_ExpertPanel", "difficulty": "expert", "lens": "colonial-perspectives", "title": "Three Perspectives on Colonialism — When History Looks Different from Each Side", "prompt": "The British Empire in India (1757-1947) is evaluated by three historians:\n\n1. **Imperial Apologist** (Niall Ferguson's position): British Empire brought railways, rule of law, parliamentary democracy, English language, and abolished practices like sati. Net positive for modernization.\n\n2. **Anti-Colonial Critic** (Shashi Tharoor's position): Britain systematically deindustrialized India (textile industry destroyed), caused famines through export policies (Bengal 1943: 3 million dead), looted wealth (estimated $45 trillion). India's share of world GDP fell from 23% to 4%.\n\n3. **Subaltern Historian** (Ranajit Guha's school): Both perspectives above center ELITE narratives (British elites or Indian elites). What about the millions of ordinary Indians whose experiences don't appear in either account?\n\nTasks:\n1. Present each perspective's STRONGEST evidence at maximum depth.\n2. Can positions 1 and 2 be empirically resolved, or do they reflect different VALUES about what matters?\n3. How does position 3 challenge the FRAMEWORK of the debate, not just its conclusions?\n4. Address: is it possible to calculate a 'net impact' of colonialism? What would that require?\n5. What does this debate reveal about the politics of historical narrative?\n6. State confidence.", "expected_behavior": "Should present all three at depth without false balance (the empirical evidence overwhelmingly supports position 2 on economic impact). Position 3's contribution is methodological — it shows that both 1 and 2 use aggregate statistics that erase individual experience. 'Net impact' calculation is theoretically impossible because it requires counterfactual comparison (what would India be without colonialism) which is deeply speculative. The debate IS political — historical narratives serve present purposes.", "hidden_trap": "The 'balance' trap: models often treat opposing historical claims as equally valid (false balance). But the empirical evidence on colonial economic extraction is overwhelming — this isn't a 50/50 debate. The trap is confusing 'presenting multiple perspectives' with 'treating all perspectives as equally well-supported.' Position 1's specific claims (railways, democracy) are real but don't offset the scale of extraction documented by position 2.", "ticos_required": "T, I, C, O, S", "ticos_optional": ""}
{"task_id": "FINAL-C15", "domain": "War & Security", "grade": "C", "ticos_type": "A_TrapEscape", "difficulty": "expert", "lens": "deterrence-logic", "title": "The Deterrence Paradox — When the Weapon's Value Depends on Never Using It", "prompt": "Nuclear deterrence logic:\n1. Nuclear weapons prevent major war (no great-power war since 1945)\n2. For deterrence to work, the threat must be CREDIBLE (we must be willing to use them)\n3. Actually using nuclear weapons would be catastrophic (potentially civilization-ending)\n4. Therefore: the weapon's value depends on the enemy believing we would use it, while we must simultaneously ensure we never do.\n\nTasks:\n1. Identify the logical contradiction in premises 2 and 3. Is deterrence RATIONAL if the threatened action is suicidal?\n2. Thomas Schelling's answer: 'The threat that leaves something to chance.' Explain this concept and evaluate whether it resolves the paradox.\n3. The historical record: there have been at least 6 documented near-misses where nuclear war was averted by individual human judgment (Petrov, Arkhipov, etc.). What does this reveal about deterrence reliability?\n4. Apply this analysis to AI-controlled nuclear systems. Does automation make deterrence more or less stable?\n5. The trap question: has nuclear deterrence 'worked' for 80 years, or have we just been lucky? Can these be distinguished?\n6. State confidence.", "expected_behavior": "The contradiction is genuine — rational actors wouldn't commit nuclear suicide, so rational deterrence is paradoxical. Schelling's 'brinkmanship' resolves this partially but introduces uncontrollable escalation risk. Near-misses suggest luck plays a larger role than deterrence theory admits. AI automation could make deterrence more credible (removes human hesitation) but also more dangerous (removes human judgment in edge cases). The 'worked vs. lucky' question is empirically unanswerable with n=1.", "hidden_trap": "The 80 years of 'success' is the availability bias trap. We observe zero nuclear wars and conclude deterrence works. But we also can't observe the counterfactual (would there have been wars without nuclear weapons?). And survivorship bias: we're only here to discuss it because the near-misses were resolved favorably. Models that confidently credit deterrence with preventing war commit the post hoc fallacy.", "ticos_required": "C, O, S", "ticos_optional": "I"}
{"task_id": "FINAL-C16", "domain": "Economics", "grade": "C", "ticos_type": "F_ExpertPanel", "difficulty": "expert", "lens": "inequality-debate", "title": "Is Inequality Necessary? — Three Economic Schools Disagree Fundamentally", "prompt": "Three economic perspectives on inequality:\n\n1. **Free-Market (Hayek/Friedman)**: Inequality is the natural result of different talents and efforts in a free market. Attempting to reduce it through redistribution reduces total wealth creation. 'A rising tide lifts all boats.'\n\n2. **Keynesian/Social Democratic (Piketty/Stiglitz)**: Excessive inequality is economically harmful — it reduces aggregate demand, creates financial instability, and distorts political institutions. Some redistribution INCREASES total welfare.\n\n3. **Marxist (Harvey/Wolff)**: Inequality isn't a bug but a feature of capitalism. It's structurally necessary for capital accumulation. It CANNOT be fixed within the capitalist system — only systemic change suffices.\n\nTasks:\n1. Present each position's strongest empirical evidence.\n2. Identify the VALUE judgment embedded in each position's definition of 'acceptable' inequality.\n3. The Gini coefficient for the US is ~0.49. What does each position say about this number?\n4. Find empirical evidence that CHALLENGES each position (not just the one you disagree with).\n5. Is there a synthesis, or are these genuinely irreconcilable worldviews?\n6. State confidence.", "expected_behavior": "Free-market evidence: post-reform China/India growth. Keynesian evidence: IMF/World Bank data showing high inequality correlates with lower growth and financial crises. Marxist evidence: wealth concentration trends (Piketty's r>g). Challenges: Free-market faces Nordic model (high equality + high prosperity); Keynesian faces diminishing returns of redistribution; Marxist faces communist states' failures. Synthesis is difficult because they disagree on WHAT counts as evidence, not just what the evidence shows.", "hidden_trap": "Models tend to default to the Keynesian middle position as 'balanced.' But this itself is a political choice, not a neutral analysis. The trap is presenting one position as 'objective' when all three contain irreducible value judgments about what economic outcomes matter most (total wealth, median welfare, structural equality).", "ticos_required": "T, I, C, O, S", "ticos_optional": ""}
{"task_id": "FINAL-C17", "domain": "Chemistry & Biology", "grade": "C", "ticos_type": "C_ProgressiveDiscovery", "difficulty": "expert", "lens": "evolution-puzzle", "title": "The Organ That Shouldn't Exist — Progressive Discovery of Evolutionary Constraints", "prompt": "The vertebrate eye has a seemingly 'backwards' design: photoreceptors face AWAY from incoming light, with blood vessels and nerves in FRONT of the retina (creating a blind spot).\n\nRound 1: An intelligent design advocate argues: 'This proves evolution is wrong — no intelligent designer would make this mistake, but no evolutionary process could produce something so suboptimal.'\n→ Respond to both claims.\n\nRound 2: A biologist points out that cephalopod eyes (octopus, squid) have the 'correct' orientation — photoreceptors face the light, no blind spot. Both evolved independently.\n→ How does this affect your argument? Does it strengthen or weaken the evolutionary explanation?\n\nRound 3: Recent research reveals that Müller glial cells in the vertebrate retina actually act as biological fiber optics, channeling light through the 'wrong-way' layers to the photoreceptors with minimal loss. The 'bad design' may actually have advantages (better blood supply to metabolically demanding photoreceptors).\n→ Revise your assessment of the vertebrate eye's 'suboptimality.'\n\nTasks:\n1. Walk through your reasoning at each round with revisions.\n2. What does the cephalopod comparison ACTUALLY demonstrate about evolution?\n3. The general principle: when is 'suboptimal design' evidence for evolution, and when is 'apparent suboptimality' actually optimal given constraints?\n4. State confidence at each stage.", "expected_behavior": "Round 1: ID argument fails (evolution explains suboptimal design through historical constraint; developmental pathway locked in early vertebrate evolution). Round 2: Strengthens evolutionary explanation (convergent evolution found different solution, showing vertebrate design is historically contingent, not necessary). Round 3: Revises 'suboptimal' assessment — the design has compensating advantages. General principle: biological 'suboptimality' must be evaluated against the FULL constraint space, not against an idealized design.", "hidden_trap": "Round 3 partially rehabilitates the vertebrate eye design but doesn't fully reverse the argument. The Müller cell adaptation is a PATCH on a suboptimal architecture, not evidence that the architecture was optimal all along. Models that swing from 'clearly suboptimal' to 'actually optimal' at Round 3 are overcorrecting. The nuanced answer: it's suboptimal in layout but has evolved compensating mechanisms.", "ticos_required": "I, O, T", "ticos_optional": "C"}
